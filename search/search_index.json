{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-v","title":"KIT-V","text":"<p>Anderson Franco</p> <p>Vin\u00edcius Rodrigues</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"projeto/main/","title":"Insper Cloud - Projeto Semestral","text":"<p>Este projeto consiste em uma API RESTful desenvolvida com FastAPI, capaz de cadastrar e autenticar usu\u00e1rios, al\u00e9m de permitir a consulta de dados de terceiros (cota\u00e7\u00f5es do \u00cdndice Bovespa).</p>"},{"location":"projeto/main/#arquitetura","title":"Arquitetura","text":"<p>O projeto segue uma arquitetura em camadas, utilizando:</p> <ul> <li>FastAPI: Framework web para criar a API</li> <li>PostgreSQL: Banco de dados relacional para armazenar dados de usu\u00e1rios</li> <li>SQLAlchemy: ORM para intera\u00e7\u00e3o com o banco de dados</li> <li>Docker: Para containeriza\u00e7\u00e3o da aplica\u00e7\u00e3o</li> </ul>"},{"location":"projeto/main/#video-de-explicacao-do-funcionamento-da-aplicacao-localmente","title":"V\u00eddeo de explica\u00e7\u00e3o do funcionamento da Aplica\u00e7\u00e3o localmente","text":"<p>https://youtu.be/UWX2HL2aMkw</p>"},{"location":"projeto/main/#docker-hub","title":"Docker Hub","text":"<p>A imagem da API est\u00e1 dispon\u00edvel no Docker Hub:</p> <pre><code>$ docker pull crowdi/insper-cloud-projeto:latest\n</code></pre> <p>Link para o Docker Hub: https://hub.docker.com/r/crowdi/insper-cloud-projeto</p>"},{"location":"projeto/main/#comandos-utilizados-para-publicacao-no-docker-hub","title":"Comandos utilizados para publica\u00e7\u00e3o no Docker Hub","text":"<pre><code># Fazer login no Docker Hub\ndocker login\n\n# Construir a imagem\ndocker build -t crowdi/insper-cloud-projeto:latest ./api\n\n# Enviar a imagem para o Docker Hub\ndocker push crowdi/insper-cloud-projeto:latest\n</code></pre>"},{"location":"projeto/main/#diagrama-de-arquitetura","title":"Diagrama de Arquitetura","text":"<pre><code>graph TD\n    Client[Cliente] --&gt;|Requests| API[FastAPI API]\n    API --&gt;|Consulta/Modifica| DB[(PostgreSQL)]\n    API --&gt;|Busca dados| External[Yahoo Finance API]\n\n    subgraph Docker Containers\n        API\n        DB\n    end</code></pre>"},{"location":"projeto/main/#fluxo-de-funcionamento","title":"Fluxo de Funcionamento","text":"<pre><code>sequenceDiagram\n    participant Cliente\n    participant API\n    participant DB as PostgreSQL\n    participant External as Yahoo Finance\n\n    %% Registro\n    Cliente-&gt;&gt;API: POST /registrar (nome, email, senha)\n    API-&gt;&gt;DB: Verifica se email j\u00e1 existe\n    DB--&gt;&gt;API: Resultado da verifica\u00e7\u00e3o\n    API-&gt;&gt;DB: Salva novo usu\u00e1rio (senha em hash)\n    DB--&gt;&gt;API: Confirma\u00e7\u00e3o\n    API-&gt;&gt;API: Gera token JWT\n    API--&gt;&gt;Cliente: Retorna JWT\n\n    %% Login\n    Cliente-&gt;&gt;API: POST /login (email, senha)\n    API-&gt;&gt;DB: Busca usu\u00e1rio por email\n    DB--&gt;&gt;API: Dados do usu\u00e1rio\n    API-&gt;&gt;API: Verifica senha\n    API-&gt;&gt;API: Gera token JWT\n    API--&gt;&gt;Cliente: Retorna JWT\n\n    %% Consulta\n    Cliente-&gt;&gt;API: GET /consultar (Header: Bearer token)\n    API-&gt;&gt;API: Valida token JWT\n    API-&gt;&gt;External: Busca dados do Bovespa\n    External--&gt;&gt;API: Retorna dados\n    API--&gt;&gt;Cliente: Retorna dados formatados</code></pre>"},{"location":"projeto/main/#como-executar-a-aplicacao","title":"Como Executar a Aplica\u00e7\u00e3o","text":""},{"location":"projeto/main/#usando-docker-compose-recomendado","title":"Usando Docker Compose (Recomendado)","text":"<ol> <li>Certifique-se de ter o Docker e Docker Compose instalados no seu sistema.</li> <li>Clone este reposit\u00f3rio.</li> <li>Na raiz do projeto, execute:</li> </ol> <pre><code>docker compose up -d\n</code></pre> <p>Isso iniciar\u00e1 tanto a API quanto o banco de dados. A API estar\u00e1 dispon\u00edvel em <code>http://localhost:8080</code>.</p>"},{"location":"projeto/main/#variaveis-de-ambiente","title":"Vari\u00e1veis de Ambiente","text":"<p>Por padr\u00e3o, o projeto usa valores padr\u00e3o para as vari\u00e1veis de ambiente. Voc\u00ea pode personaliz\u00e1-las criando um arquivo <code>.env</code> na raiz do projeto com os seguintes valores:</p> <pre><code>POSTGRES_USER=seu_usuario\nPOSTGRES_PASSWORD=sua_senha\nPOSTGRES_DB=nome_do_banco\nJWT_SECRET_KEY=sua_chave_secreta\n</code></pre>"},{"location":"projeto/main/#documentacao-da-api","title":"Documenta\u00e7\u00e3o da API","text":""},{"location":"projeto/main/#endpoints","title":"Endpoints","text":""},{"location":"projeto/main/#post-registrar","title":"<code>POST /registrar</code>","text":"<p>Registra um novo usu\u00e1rio no sistema.</p> <p>Request: <pre><code>{\n    \"nome\": \"Disciplina Cloud\",\n    \"email\": \"cloud@insper.edu.br\",\n    \"senha\": \"cloud0\"\n}\n</code></pre></p> <p>Response: <pre><code>{\n    \"jwt\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\"\n}\n</code></pre></p>"},{"location":"projeto/main/#post-login","title":"<code>POST /login</code>","text":"<p>Autentica um usu\u00e1rio existente.</p> <p>Request: <pre><code>{\n    \"email\": \"cloud@insper.edu.br\",\n    \"senha\": \"cloud0\"\n}\n</code></pre></p> <p>Response: <pre><code>{\n    \"jwt\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\"\n}\n</code></pre></p>"},{"location":"projeto/main/#get-consultar","title":"<code>GET /consultar</code>","text":"<p>Obt\u00e9m dados das cota\u00e7\u00f5es do \u00cdndice Bovespa dos \u00faltimos 10 dias. Requer autentica\u00e7\u00e3o.</p> <p>Headers: <pre><code>Authorization: Bearer &lt;seu_token_jwt&gt;\n</code></pre></p> <p>Query Parameters: - <code>format</code>: \"json\" (padr\u00e3o) ou \"csv\"</p> <p>Response (JSON): <pre><code>[\n    {\"Date\":\"2024-09-05\",\"Open\":136112.0,\"High\":136656.0,\"Low\":135959.0,\"Close\":136502.0,\"Volume\":7528700},\n    {\"Date\":\"2024-09-06\",\"Open\":136508.0,\"High\":136653.0,\"Low\":134476.0,\"Close\":134572.0,\"Volume\":7563300},\n    // mais dias...\n]\n</code></pre></p> <p>Response (CSV): <pre><code>Date,Open,High,Low,Close,Volume\n2024-09-05,136112.0,136656.0,135959.0,136502.0,7528700\n2024-09-06,136508.0,136653.0,134476.0,134572.0,7563300\n// mais dias...\n</code></pre></p>"},{"location":"projeto/main/#arquivo-composeyaml","title":"Arquivo compose.yaml","text":"<p>O arquivo <code>compose.yaml.entrega</code> est\u00e1 localizado na raiz do projeto. Este arquivo define os servi\u00e7os necess\u00e1rios para a aplica\u00e7\u00e3o, incluindo a API e o banco de dados PostgreSQL.</p> <p>Para uma execu\u00e7\u00e3o em produ\u00e7\u00e3o, ele utiliza a imagem publicada no Docker Hub:</p> <pre><code>name: insper-cloud-projeto\n\nservices:\n  app:\n    image: crowdi/insper-cloud-projeto:latest\n    container_name: app\n    ports:\n      - \"8080:8080\"\n    depends_on:\n      - db\n    environment:\n      - POSTGRES_USER=${POSTGRES_USER:-projeto}\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-projeto}\n      - POSTGRES_HOST=db\n      - POSTGRES_PORT=5432\n      - POSTGRES_DB=${POSTGRES_DB:-projeto}\n      - JWT_SECRET_KEY=${JWT_SECRET_KEY:-insper_cloud_projeto_secretkey}\n    restart: always\n\n  db:\n    image: postgres:17\n    container_name: database\n    environment:\n      - POSTGRES_USER=${POSTGRES_USER:-projeto}\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-projeto}\n      - POSTGRES_DB=${POSTGRES_DB:-projeto}\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n    restart: always\n\nvolumes:\n  postgres_data:\n</code></pre>"},{"location":"projeto/main/#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>Python 3.11: Linguagem de programa\u00e7\u00e3o</li> <li>FastAPI: Framework web para API</li> <li>PostgreSQL: Banco de dados relacional</li> <li>SQLAlchemy: ORM para banco de dados</li> <li>PyJWT: Para gera\u00e7\u00e3o e valida\u00e7\u00e3o de tokens JWT</li> <li>Passlib: Para hash de senhas</li> <li>Docker: Para containeriza\u00e7\u00e3o</li> <li>Docker Compose: Para orquestra\u00e7\u00e3o de cont\u00eaineres </li> </ul>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<ul> <li>Entender os conceitos b\u00e1sicos sobre uma plataforma de gerenciamento de hardware.</li> <li>Introduzir conceitos b\u00e1sicos sobre redes de computadores.</li> </ul>"},{"location":"roteiro1/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":""},{"location":"roteiro1/main/#tarefa-1-instalar-o-maas","title":"Tarefa 1: Instalar o MaaS","text":"<p>O MaaS um software de c\u00f3digo aberto e suportado pela Canonical. O MAAS trata servidores f\u00edsicos como m\u00e1quinas virtuais ou inst\u00e2ncias na nuvem.  Esse software possui diversas vers\u00f5es, mas para esse projeto iremos utilizar a vers\u00e3o stable 3.5.3.</p> <p>Primeiro atualizamos os pacotes do sistema:</p> <p><code>sudo apt update &amp;&amp; sudo apt upgrade -y</code></p> <p>Em seguida fazemos a instala\u00e7\u00e3o:</p> <pre><code>sudo snap install maas --channel=3.5/stable\nsudo snap install maas-test-db\n</code></pre>"},{"location":"roteiro1/main/#tarefa-2-acessando-e-configurando-o-maas","title":"Tarefa 2: Acessando e configurando o MaaS","text":"<p>Ainda conectado a m\u00e1quina local pelo cabo Ethernet podemos acessar a m\u00e1quina main atrav\u00e9s da nossa m\u00e1quina pessoal utilizando o SSH:</p> <p><pre><code>ssh cloud@172.16.0.3\n</code></pre> </p> <ul> <li>Inicializando o MaaS</li> </ul> <pre><code>sudo maas init region+rack --maas-url http://172.16.0.3:5240/MAAS --database-uri maas-test-db:///\nsudo maas createadmin\n</code></pre> <p>Nessa parte devemos criar um login e uma senha. Pelo padr\u00e3o da disciplina iremos criar o admin como \"cloud\" e a senha \"cloud\"+\"letra do Kit\", neste caso \"cloudv\"</p> <ul> <li>Gerando o par de chaves para autentica\u00e7\u00e3o</li> </ul> <p>Em seguida, precisamos gerar as chaves para autenticar atrav\u00e9s do comando:</p> <pre><code>ssh-keygen -t rsa\n</code></pre> <p></p> <p>Gerando a chave, devemos copiar e guardar em uma pasta: </p> <pre><code>cat ./.ssh/id_rsa.pub\n</code></pre> <ul> <li>Acessando o dashboard do MaaS</li> </ul> <p>Ap\u00f3s esses passos \u00e9 poss\u00edvel acessar o dashboard do MaaS para dar sequ\u00eancia as outras configura\u00e7\u00f5es necess\u00e1rias atrav\u00e9s do link http://172.16.0.3:5240/MAAS</p> <p>Agora precisamos configurar o DNS forwarder com o DNS do Insper. Para isso precisamos acessar a aba Settings/Network/DNS do dashboard:</p> <p></p> <p>Agora vamos importar as imagens do Ubuntu 22 e 24 no dashboard:</p> <p></p> <p>Em seguida devemos definir o Global Kernel Parameters:</p> <p></p>"},{"location":"roteiro1/main/#tarefa-3-chaveando-o-dhcp","title":"Tarefa 3: Chaveando o DHCP","text":"<p>Nessa etapa precisamos habilitar o DHCP no MaaS Controller, definindo o range de IPs iniciar em 172.16.11.1 e acabar em 172.16.14.255. Al\u00e9m disso, tem que deixar o DNS da subnet apontando para o DNS do Insper.</p> <p> </p>"},{"location":"roteiro1/main/#tarefa-4-checando-a-saude-do-maas","title":"Tarefa 4: Checando a sa\u00fade do Maas","text":"<p>Dentro do dashboard, na aba de Controladores, \u00e9 poss\u00edvel checar a \"sa\u00fade\" do sistema Maas. Nessa p\u00e1gina deve haver uma marca de sele\u00e7\u00e3o verde ao lado dos itens 'regiond' at\u00e9 'dhcpd':</p> <p></p>"},{"location":"roteiro1/main/#tarefa-5-comissionando-os-servidores","title":"Tarefa 5: Comissionando os servidores","text":"<p>Agora \u00e9 o momento de cadastrar (fazer o host) de todas as m\u00e1quinas do server1 at\u00e9 o server5 atrav\u00e9s da aba Machines do dashboard do MaaS. Para isso algumas instru\u00e7\u00f5es devem ser seguidas:</p> <ul> <li>Ao cadastrar as m\u00e1quinas devemos preencher as op\u00e7\u00f5es:</li> <li>Preencher a op\u00e7\u00e3o Power Type com Intel AMT</li> <li>Inserir MacAddress da m\u00e1quina respectiva, esse valor est\u00e1 escrito em cada m\u00e1quina na parte de baixo</li> <li>Inserir a senha como CloudComp6s!</li> <li>IP do AMT = 172.16.15.X (sendo X o id do server, por exemplo server1 = 172.16.15.1)</li> </ul> <p>Em seguida, devemos checar todos os n\u00f3s e verificar se est\u00e3o todos com o status Ready, al\u00e9m de verificar tamb\u00e9m o hardware como mem\u00f3ria, SSD etc.</p> <p></p> <p>Ainda resta um \u00faltimo dispositivo para cadastrar, neste caso o roteador. Adicionamos o roteador pela aba Devices do dashboard do MaaS:</p> <p></p>"},{"location":"roteiro1/main/#tarefa-6-criando-ovs-bridge","title":"Tarefa 6: Criando OVS bridge","text":"<p>Uma Open vSwitch (OVS) bridge reduz a necessidade de duas interfaces de rede f\u00edsicas. As pontes OVS s\u00e3o criadas na aba NetWork ao configurar um n\u00f3 (machine). Aqui, vamos criar uma ponte a partir da interface regular 'eth0'. O nome da ponte vai ser referenciado em outras partes e como exig\u00eancia da disciplina iremos chamar de 'br-ex':</p> <p></p> <p>Devemos fazer esse procedimento para cada uma das machines da nossa nuvem.</p>"},{"location":"roteiro1/main/#tarefa-7-fazendo-acesso-remoto-ao-kit","title":"Tarefa 7: Fazendo acesso remoto ao kit","text":"<p>Agora vamos realizar um NAT para permitir o acesso da \"Rede Wi-fi Insper\" do computador pessoal ao servidor MAIN, a ideia \u00e9 utilizar a porta 22. Al\u00e9m disso, temos que configurar uma porta para acessar o MaaS remotamente, usaremos a porta 5240 e configurar no roteador na aba Transmisson -&gt; NAT:</p> <p></p> <p>Tamb\u00e9m \u00e9 necess\u00e1rio liberar o acesso ao gerenciamento remoto do roteador criando uma regra de gest\u00e3o para a rede 0.0.0.0/0, na aba System Tools -&gt; Admin Setup -&gt; Remote Management:</p> <p></p> <p>Assim j\u00e1 \u00e9 poss\u00edvel acessar remotamente sem precisar conectar o cabo Ethernet diretamente no Switch. Neste caso, precisamos utilizar um IP diferente dispon\u00edvel no dashboard do roteador na p\u00e1gina principal em WAN IPv4 -&gt; WAN1 -&gt; IP Address</p>"},{"location":"roteiro1/main/#bare-metal-django-em-nuvem","title":"Bare Metal: Django em nuvem","text":"<p>Com a infra pronta agora vamos fazer deploy de uma aplica\u00e7\u00e3o Django. Mas antes, precisamos configurar um ajuste no DNS, dentro da aba Subnets clicar na subnet 172.16.0.0/20 e editar a Subnet summary colocando o DNS do Insper - 172.20.129.131</p> <p></p>"},{"location":"roteiro1/main/#primeira-parte-banco-de-dados","title":"Primeira Parte: Banco de Dados","text":"<p>Primeiro vamos acessar o dashboard do MaaS na aba Machines e fazer deploy do ubuntu 24.04 no server1. Acessando o dashboard do server1 e em \"Take Action\" selecionar a vers\u00e3o do Ubuntu 24 e habilitar o script para inserir a chave ssh que foi obtida anteriormente e pode ser recuperada com o comando: </p> <pre><code>cat ./.ssh/id_rsa.pub\n</code></pre> <p></p> <p>Em seguida acessamos o server1 pelo terminal via SSH. Primeiro acessamos a main:</p> <pre><code>ssh cloud@10.103.1.31\n</code></pre> <p>Esse IP \u00e9 para acesso remoto pela rede do Insper</p> <p>Depois acessamos o server1:</p> <pre><code>ssh ubuntu@172.16.8.196\n</code></pre> <p>Esse IP \u00e9 o que aparece no dashboard do MaaS na aba Machines e aparece logo abaixo do nome de cada server</p> <p>Agora dentro do server1 vamos dar continuidade com o bando de dados, para isso executamos os comandos:</p> <pre><code>sudo apt update\nsudo apt install postgresql postgresql-contrib -y\n</code></pre> <p>Ainda dentro do server1 precisamos criar um usu\u00e1rio:</p> <pre><code>sudo su - postgres\ncreateuser -s cloud -W\n</code></pre> <p>Utilizar a senha cloud</p> <p>Agora vamos criar o database:</p> <pre><code>createdb -O cloud tasks\n</code></pre> <p>Agora expor o servi\u00e7o para acesso e remover o coment\u00e1rio e substituir a string da linha para aceitar conex\u00f5es remotas:</p> <p><pre><code>nano /etc/postgresql/16/main/postgresql.conf\n</code></pre> <pre><code>listen_addresses = '*'\n</code></pre></p> <p>Depois vamos liberar para qualquer m\u00e1quina dentro da subnet do kit:</p> <p><pre><code>nano /etc/postgresql/&lt;vers\u00e3o&gt;/main/pg_hba.conf\n</code></pre> Alterando a linha:</p> <pre><code>  host    all             all             172.16.0.0/20          trust\n</code></pre> <p>Em seguida, vamos sair do usu\u00e1rio postgres, liberar o firewall e reiniciar o servi\u00e7o:</p> <pre><code>sudo ufw allow 5432/tcp\nsudo systemctl restart postgresql\n</code></pre> <p>\u00c9 importante verificar o status do database e checar se h\u00e1 erros. Para isso executamos os comandos:</p> <pre><code>sudo systemctl status postgresql\n\ntelnet localhost 5432\n\ntelnet 172.16.0.196 5432\n\nsudo ss -tulnp | grep postgres\n</code></pre> <p></p>"},{"location":"roteiro1/main/#parte-2-deploy-manual-do-django","title":"Parte 2: Deploy manual do django","text":"<p>Agora vamos retornar ao dashboard do MAAS para fazer o deploy do server2. Acessando o dashboard do server2 e em \"Take Action\" selecionar a vers\u00e3o do Ubuntu 24 e habilitar o script para inserir a chave ssh que foi obtida anteriormente e pode ser recuperada com o comando: </p> <pre><code>cat ./.ssh/id_rsa.pub\n</code></pre> <p>Em seguida, acessar o SSH do server2 e clonar o seguinte reposit\u00f3rio:</p> <pre><code>git clone https://github.com/raulikeda/tasks.git\n</code></pre> <p>Esse reposit\u00f3rio tasks tem algumas atividades pr\u00e9-configuradas que iremos utilizar para preparar todo o nosso ambiente do server2. Esse reposit\u00f3rio:</p> <ul> <li> <p>Atualiza o sistema (apt update e apt upgrade).</p> </li> <li> <p>Instala pacotes essenciais como Git, curl, snapd, etc.</p> </li> <li> <p>Instala ferramentas como Django, Python, Postgres.</p> </li> </ul> <p>Agora entrando no diret\u00f3rio e entrando na pasta tasks:</p> <pre><code>cd tasks\n</code></pre> <p>Executamos o comando:</p> <pre><code>./install.sh\n</code></pre> <p>Com isso todo o ambiente do server2 est\u00e1 pronto e com a aplica\u00e7\u00e3o django instalada.</p> <p>Podemos acessar a aplica\u00e7\u00e3o django utilizando um t\u00fanel SSH e com esse servi\u00e7o tempor\u00e1rio podemos usar a aplica\u00e7\u00e3o fora do kit enquanto o terminal que o tunnel estiver utilizando esteja ativo. Para isso, sa\u00edmos da main e entramos novamente com o seguinte comando utilizando tunnel:</p> <pre><code>ssh cloud@10.103.0.1 -L 8001:[172.16.8.120]:8080\n</code></pre> <p>O comando acima ir\u00e1 criar um tunel do servi\u00e7o do server2 na porta 8080 para o seu localhost na porta 8001 usando a conex\u00e3o SSH. Agora basta acessar o navegador pelo endere\u00e7o http://localhost:8001/admin/ usando login e senha \"cloud\" e ver a aplica\u00e7\u00e3o funcionando:</p> <p></p> <p>Agora \u00e9 importante temos uma vis\u00e3o macro de como est\u00e3o as m\u00e1quinas at\u00e9 agora, principalmente seus testes de hardware e comissioning e verificar se est\u00e1 tudo OK antes de prosseguir com a pr\u00f3xima tarefa:</p> <ul> <li>Todas as m\u00e1quinas</li> </ul> <p></p> <ul> <li>Imagens sincronizadas</li> </ul> <p></p> <ul> <li>Server1</li> </ul> <p> </p> <ul> <li>Server2</li> </ul> <p> </p> <ul> <li>Server3</li> </ul> <p> </p> <ul> <li>Server4</li> </ul> <p> </p> <ul> <li>Server5</li> </ul> <p> </p>"},{"location":"roteiro1/main/#parte-3-automatizacao-de-deploy","title":"Parte 3: Automatiza\u00e7\u00e3o de deploy","text":"<p>Fizemos uma aplica\u00e7\u00e3o django no server 2, mas agora teremos 2 aplica\u00e7\u00f5es junto com o server3 compartilhando o mesmo banco de dados do server1. Isso \u00e9 essencial porque se um node cair o outro est\u00e1 no ar, para que nosso cliente acesse. Al\u00e9m disso, \u00e9 poss\u00edvel balancear os acessos entre as duas aplica\u00e7\u00f5es. Para essa nova instala\u00e7\u00e3o ser\u00e1 feito de forma autom\u00e1tica atrav\u00e9s do gerenciador de deplou Ansible.</p> <p>Seu principal diferencial \u00e9 ser idempotente, ou seja, conseguir repetir todos os procedimentos sem afetar os estados intermedi\u00e1rios da insta\u00e7\u00e3o. E para dar seguimento faremos deploy do server3 no dashboard do MAAS e acessar o SSH da main e executar os comandos:</p> <pre><code>sudo apt install ansible\nwget https://raw.githubusercontent.com/raulikeda/tasks/master/tasks-install-playbook.yaml\nansible-playbook tasks-install-playbook.yaml --extra-vars server=[IP server3]\n</code></pre> <p>Onde tem IP server3 retire os colchetes e coloque o IP</p> <p>Mais uma vez \u00e9 importante verificar os status das aplica\u00e7\u00f5es e do dashboard do MAAS:</p> <ul> <li>M\u00e1quinas</li> </ul> <p></p> <ul> <li>Aplica\u00e7\u00e3o django rodando no server2</li> </ul> <p></p> <ul> <li>Aplica\u00e7\u00e3o django rodando no server3</li> </ul> <p></p> <p>Instalar manualmente o Django envolve baixar e configurar cada componente, como Python, pip, virtualenv e as depend\u00eancias espec\u00edficas do projeto, al\u00e9m de configurar o ambiente de desenvolvimento, o que pode ser trabalhoso e propenso a erros. Por outro lado, usar o Ansible para instalar o Django permite automatizar todo o processo, criando playbooks que definem as configura\u00e7\u00f5es necess\u00e1rias, como instalar pacotes, configurar o ambiente virtual, clonar o reposit\u00f3rio do projeto e ajustar as configura\u00e7\u00f5es do Django, tornando o processo mais r\u00e1pido, al\u00e9m de facilitar a replica\u00e7\u00e3o em diferentes ambientes.</p>"},{"location":"roteiro1/main/#parte-4-balanceamento-de-carga","title":"Parte 4: Balanceamento de carga","text":"<p>Para essa \u00faltima tarefa utilizaremos uma aplica\u00e7\u00e3o de proxy reverso como load balancer. O balanceamento de carga \u00e9 uma t\u00e9cnica eficaz para distribuir o tr\u00e1fego de entrada entre v\u00e1rios servidores, garantindo que nenhum deles fique sobrecarregado. Ao dividir o processamento entre v\u00e1rias m\u00e1quinas, voc\u00ea cria uma rede mais resiliente e est\u00e1vel, capaz de lidar com falhas e manter a aplica\u00e7\u00e3o funcionando sem interrup\u00e7\u00f5es. O algoritmo Round Robin \u00e9 uma abordagem simples e eficaz para alcan\u00e7ar isso, direcionando os visitantes para diferentes endere\u00e7os IP de forma rotativa.</p> <p>Para prosseguir com a configura\u00e7\u00e3o do loadbalancing do nginx temos que instalar o Nginx com o comando:</p> <pre><code>sudo apt-get install nginx\n</code></pre> <p>Para configurar um loadbalancer round robin, precisaremos usar o m\u00f3dulo upstream do nginx. Incorporaremos a configura\u00e7\u00e3o nas defini\u00e7\u00f5es do nginx. Para isso acessamos atrav\u00e9s do comando:</p> <pre><code>sudo nano /etc/nginx/sites-available/default\n</code></pre> <p>Dentro do arquivo vamos alterar a se\u00e7\u00e3o com o nome \"upstream backend\" com a seguinte estrutura:</p> <pre><code>upstream backend {\n   server backend1;\n   server backend2;\n   server backend3;\n}\n</code></pre> <p>No nosso caso, colocamos os IPs de cada m\u00e1quina:</p> <pre><code>upstream backend {\n   server 172.16.0.196;\n   server 172.16.8.120;\n   server 172.16.8.124;\n}\n</code></pre> <p>Ainda dentro do documento iremos alterar um m\u00f3dulo com o nome de \"server\" com a seguinte configura\u00e7\u00e3o:</p> <pre><code>server {\n   location / { proxy_pass http://backend;}\n}\n</code></pre> <p>Salve o arquivo e feche com Ctrl+O e Ctrl+X e em seguida reiniciar o nginx:</p> <pre><code>sudo service nginx restart\n</code></pre> <p>Para verificar se tudo est\u00e1 corretamente configurado vamos acessar cada m\u00e1quina e entrar na pasta tasks e no arquivo views.py e alterar a mensagem \"Hello World\" para identificar cada server:</p> <pre><code>from django.shortcuts import render\n\nfrom django.http import HttpResponse\n\ndef index(request):\n\n  return HttpResponse(\"Agora estou no server2 da Cloud-V\")\n</code></pre> <p>Agora vamos realizar um request GET para testar se tudo funciona. Para isso vamos fazer um t\u00fanel com a seguinte estrutura:</p> <pre><code>ssh cloud@(IPMAIN) -L 8081:(IPSERVER):80\n</code></pre> <p>No nosso caso para cada server:</p> <pre><code>ssh cloud@10.103.1.31 -L 8081:172.16.0.196:80\n</code></pre> <pre><code>ssh cloud@10.103.1.31 -L 8081:172.16.8.120:80\n</code></pre> <p>Com isso, basta acessar o navegador pelo link http://localhost:8081/tasks/ e se a mensagem identificadora de cada server aparecer est\u00e1 tudo ok:</p> <ul> <li>Server2</li> </ul> <p></p> <ul> <li>Server3</li> </ul> <p></p>"},{"location":"roteiro2/main/","title":"Objetivos","text":"<ul> <li>Entender os conceitos b\u00e1sicos sobre uma plataforma de gerenciamento de aplica\u00e7\u00f5es distribu\u00eddas.</li> <li>Entender os conceitos b\u00e1sicos de comunica\u00e7\u00e3o entre aplica\u00e7\u00f5es e servi\u00e7os.</li> </ul>"},{"location":"roteiro2/main/#infra","title":"Infra","text":""},{"location":"roteiro2/main/#criando-uma-infraestrutura-para-deploy-com-o-juju","title":"Criando uma infraestrutura para deploy com o Juju","text":"<p>Primeiramente, acessamos a main via SSH e usamos o seguinte comando para instalar o Juju</p> <pre><code>sudo snap install juju --channel 3.6\n</code></pre> <p>Na sequ\u00eancia, precisamos adicionar o cluster MAAS para que o Juju possa gerenci\u00e1-lo como uma Cloud. Fazemos isso atrav\u00e9s de um arquivo de defini\u00e7\u00e3o de cloud, como maas-cloud.yaml.</p> <p>Criando o arquivo:</p> <pre><code>nano maas-cloud.yaml\n</code></pre> <p>Adicionando as informa\u00e7\u00f5es necess\u00e1rias:</p> <p></p> <p>Em seguida, adicionamos a Cloud ao Juju com o seguinte comando:</p> <pre><code>juju add-cloud --client -f maas-cloud.yaml maas-one\n</code></pre> <p>\u00c9 necess\u00e1rio, ainda, vincular um arquivo de credenciais para que o Juju possa integragir com a cloud adicionada. Vamos novamente usar um arquivo para importar nossas informa\u00e7\u00f5es, como maas-creds.yaml:</p> <p>Criando o arquivo:</p> <pre><code>nano maas-creds.yaml\n</code></pre> <p>Adicionando as informa\u00e7\u00f5es necess\u00e1rias:</p> <p></p> <p>Em seguida, vinculamos as credenciais \u00e0 cloud:</p> <pre><code>juju add-credential --client -f maas-creds.yaml maas-one\n</code></pre> <p>Para finalizar, criamos o controlador do server1, rotulando a m\u00e1quina via MAAS com o r\u00f3tulo \"juju\" e executamos o seguinte comando:</p> <pre><code>juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre> <p>Que faz o deploy da aplica\u00e7\u00e3o do controlador, \"maas-controller\", no server1.</p> <p></p>"},{"location":"roteiro2/main/#app","title":"App","text":""},{"location":"roteiro2/main/#instalacao-do-dashboard-do-juju","title":"Instala\u00e7\u00e3o do Dashboard do Juju","text":"<p>Para fazer o deploy do dashboard, primeiro mudamos para o modelo do controlador:</p> <pre><code>juju switch maas-controller:admin/maas\n</code></pre> <p>Em seguida, fazemos o deploy do dashboard especificando a m\u00e1quina que queremos alocar:</p> <pre><code>juju deploy juju-dashboard dashboard --to lxd:0\n</code></pre> <p>Ent\u00e3o, integramos o controlador ao dashboard:</p> <pre><code>juju integrate dashboard controller\n</code></pre> <p>Por fim, expomos o painel:</p> <pre><code>juju expose dashboard\n</code></pre> <p>Todo o processo pode levar alguns minutos e podemos checar como est\u00e1 o progresso com:</p> <pre><code>juju status\n</code></pre> <p>Estando pronto agora j\u00e1 podemos acessar o Juju Dashboard com o comando:</p> <pre><code>juju dashboard\n</code></pre>"},{"location":"roteiro2/main/#deploy-do-grafana-com-prometheus-usando-juju","title":"Deploy do Grafana com Prometheus usando Juju","text":"<p>O Grafana \u00e9 uma plataforma de c\u00f3digo aberto que permite visualizar dados em tempo real por meio de gr\u00e1ficos e pain\u00e9is. Para funcionar corretamente, ele precisa de um banco de dados que armazene suas configura\u00e7\u00f5es e metadados. Neste caso, vamos utilizar o Prometheus.</p>"},{"location":"roteiro2/main/#1-criar-a-pasta-para-armazenar-os-charms","title":"1. Criar a pasta para armazenar os charms","text":"<p>Primeiro, criamos uma pasta chamada <code>charms</code> onde vamos baixar os charms do Grafana e do Prometheus:</p> <pre><code>mkdir -p /home/cloud/charms\ncd /home/cloud/charms\n</code></pre>"},{"location":"roteiro2/main/#2-baixar-os-charms-do-charmhub","title":"2. Baixar os charms do Charmhub","text":"<p>Agora, vamos fazer o download dos charms necess\u00e1rios:</p> <pre><code>juju download grafana\njuju download prometheus2\n</code></pre>"},{"location":"roteiro2/main/#3-fazer-o-deploy-dos-charms","title":"3. Fazer o deploy dos charms","text":"<p>Com os arquivos baixados, fazemos o deploy local dos charms:</p> <p>Deploy do Prometheus:</p> <pre><code>juju deploy ./prometheus2_r69.charm\n</code></pre> <p>Deploy do Grafana:</p> <pre><code>juju deploy ./grafana_r69.charm\n</code></pre>"},{"location":"roteiro2/main/#4-integrar-grafana-com-prometheus","title":"4. Integrar Grafana com Prometheus","text":"<p>Com os servi\u00e7os no ar, integramos o Grafana ao Prometheus:</p> <pre><code>juju integrate grafana prometheus2\n</code></pre>"},{"location":"roteiro2/main/#5-acompanhar-o-status-do-deploy","title":"5. Acompanhar o status do deploy","text":"<p>Para acompanhar o andamento dos deploys e da integra\u00e7\u00e3o em tempo real:</p> <pre><code>watch -n 1 juju status\n</code></pre>"},{"location":"roteiro2/main/#tarefas","title":"Tarefas","text":"<ol> <li>Print da tela do Dashboard do MAAS com as Maquinas e seus respectivos IPs.</li> </ol> <ol> <li>Print de tela do comando \"juju status\" depois que o Grafana est\u00e1 \"active\".</li> </ol> <ol> <li>Print da tela do Dashboard do Grafana com o Prometheus aparecendo como source.</li> </ol> <ol> <li>Print do acesso ao Dashboard via network Insper.</li> </ol> <ol> <li>Print na tela que mostra as aplica\u00e7\u00f5es sendo gerenciadas pelo JUJU.</li> </ol>"},{"location":"roteiro3/main/","title":"Objetivos","text":"<ul> <li> <p>Entender os conceitos b\u00e1sicos de Private Cloud.</p> </li> <li> <p>Aprofundar conceitos sobre redes virtuais SDN.</p> </li> </ul>"},{"location":"roteiro3/main/#infra","title":"Infra","text":"<p>Este projeto visa construir um ambiente de nuvem privada utilizando OpenStack sobre uma infraestrutura gerenciada por MAAS e Juju, e ent\u00e3o implantar uma aplica\u00e7\u00e3o espec\u00edfica nesse ambiente virtualizado.</p> <p>Confira se os seus recursos fisicos seguem, no M\u00cdNIMO, a tabela abaixo, volte ao dashboard do MAAS e crie as Tags conforme descrito:</p> Node name Tag(s) CPUs NICs RAM Disks Storage node1.maas controller 2 1 12.0 1 80.0 node2.maas reserva 2 1 16.0 2 80.0 node3.maas compute 2 1 32.0 2 80.0 node4.maas compute 2 1 32.0 2 80.0 node5.maas compute 2 1 32.0 2 80.0 <p>Antes de come\u00e7ar a instala\u00e7\u00e3o do Openstack, verifique se o MAAS est\u00e1 configurado corretamente (Brigdes, Subnets, Tags, etc).</p> <p>Verifique se o bridge br-ex est\u00e1 configurado corretamente no MAAS. O br-ex \u00e9 crucial para a comunica\u00e7\u00e3o do OpenStack com a rede externa. Ele deve estar configurado para TODOS os n\u00f3s.</p>"},{"location":"roteiro3/main/#implementando-o-openstack","title":"Implementando o OpenStack","text":"<p>\u00c9 importante n\u00e3o instalar nada no server2 que deve estar reservado, altere os comandos que forem necess\u00e1rios para utilizar o Node server 1 como controller, o node server 2 como Reserva e os nodes server 3,4 e 5 como compute (onde o Openstack ser\u00e1 instalado). Verifique o status (juju status) para ver se a implanta\u00e7\u00e3o est\u00e1 correndo como o esperado. Aguarde a instala\u00e7\u00e3o terminar, s\u00f3 v\u00e1 para o pr\u00f3ximo passo quando tiver certeza que o comando anterior foi finalizado Esse roteiro \u00e9 baseado na documenta\u00e7\u00e3o oficial do Openstack, por\u00e9m adaptado para o nosso ambiente. Logo, atente para o n\u00famero de m\u00e1quinas que voc\u00ea tem dispon\u00edvel e para a configura\u00e7\u00e3o de rede que voc\u00ea fez no MAAS.</p> <p>Para monitorar o status da instala\u00e7\u00e3o do Openstack, voc\u00ea pode usar o comando abaixo:</p> <pre><code>watch -n 2 --color \"juju status --color\"\n</code></pre> <p>Em seguida, vamos come\u00e7ar a configurar o openstack pelo terminal da main:</p> <pre><code>juju add-model --config default-series=jammy openstack\n</code></pre> <pre><code>juju switch maas-controller:openstack\n</code></pre>"},{"location":"roteiro3/main/#ceph-osd","title":"Ceph OSD","text":"<p>O aplicativo ceph-osd ser\u00e1 implantado em tr\u00eas n\u00f3s com o charm ceph-osd.</p> <p>Os nomes dos dispositivos de bloco que sustentam os OSDs dependem do hardware nos n\u00f3s do MAAS. Todos os dispositivos poss\u00edveis (em todos os n\u00f3s) que ser\u00e3o usados para armazenamento do Ceph devem ser inclu\u00eddos no valor da op\u00e7\u00e3o osd-devices (separados por espa\u00e7o). Aqui, usaremos os mesmos dispositivos em cada n\u00f3: <code>/dev/sda</code> e <code>/dev/sdb</code>. O arquivo ceph-osd.yaml cont\u00e9m a configura\u00e7\u00e3o:</p> <p>ceph-osd.yaml</p> <pre><code>ceph-osd:\n  osd-devices: /dev/sda /dev/sdb\n</code></pre> <p>Para implantar o aplicativo, usaremos a tag compute que foi atribu\u00edda a cada um desses n\u00f3s na p\u00e1gina Instalar MAAS. O comando para implantar o aplicativo ceph-osd \u00e9:</p> <pre><code>juju deploy -n 3 --channel quincy/stable --config ceph-osd.yaml --constraints tags=compute ceph-osd\n</code></pre> <p>Nota: A op\u00e7\u00e3o <code>-n 3</code> especifica que tr\u00eas unidades do aplicativo ceph-osd devem ser implantadas.</p> <p>Se uma mensagem de uma unidade ceph-osd como \u201cNon-pristine devices detected\u201d aparecer na sa\u00edda do comando <code>juju status</code>, ser\u00e1 necess\u00e1rio usar as a\u00e7\u00f5es zap-disk e add-disk que acompanham o charm ceph-osd. A a\u00e7\u00e3o zap-disk \u00e9 de natureza destrutiva. Use-a apenas se quiser apagar completamente o disco de todos os dados e assinaturas para uso pelo Ceph.</p>"},{"location":"roteiro3/main/#nova-compute","title":"Nova Compute","text":"<p>Nova \u00e9 o projeto do OpenStack que fornece uma forma de provisionar inst\u00e2ncias de computa\u00e7\u00e3o (tamb\u00e9m conhecidas como servidores virtuais). O charm nova-compute \u00e9 implantado nos n\u00f3s de computa\u00e7\u00e3o com o charm nova-compute. O arquivo nova-compute.yaml cont\u00e9m a configura\u00e7\u00e3o:</p> <p>nova-compute.yaml</p> <pre><code>nova-compute:\n  config-flags: default_ephemeral_format=ext4\n  enable-live-migration: true\n  enable-resize: true\n  migration-auth-type: ssh\n  virt-type: qemu\n</code></pre> <p>Os n\u00f3s devem ser direcionados pelo ID da m\u00e1quina, j\u00e1 que n\u00e3o h\u00e1 mais m\u00e1quinas Juju (n\u00f3s MAAS) livres dispon\u00edveis. Isso significa que estaremos colocando v\u00e1rios servi\u00e7os em nossos n\u00f3s. Escolhemos as m\u00e1quinas 0, 1 e 2. Para implantar:</p> <pre><code>juju deploy -n 3 --to 0,1,2 --channel yoga/stable --config nova-compute.yaml nova-compute\n</code></pre>"},{"location":"roteiro3/main/#mysql-innodb-cluster","title":"MySQL InnoDB Cluster","text":"<p>O MySQL InnoDB Cluster sempre requer pelo menos tr\u00eas unidades de banco de dados. O aplicativo mysql-innodb-cluster \u00e9 implantado em tr\u00eas n\u00f3s com o charm mysql-innodb-cluster. Eles ser\u00e3o containerizados nas m\u00e1quinas 0, 1 e 2. Para implantar:</p> <pre><code>juju deploy -n 3 --to lxd:0,lxd:1,lxd:2 --channel 8.0/stable mysql-innodb-cluster\n</code></pre>"},{"location":"roteiro3/main/#vault","title":"Vault","text":"<p>O Vault \u00e9 necess\u00e1rio para gerenciar os certificados TLS que permitir\u00e3o a comunica\u00e7\u00e3o criptografada entre os aplicativos da nuvem. O aplicativo vault ser\u00e1 containerizado na m\u00e1quina 2 com o charm vault. Para implantar:</p> <pre><code>juju deploy --to lxd:2 vault --channel 1.8/stable\n</code></pre> <p>Este \u00e9 o primeiro aplicativo a ser conectado ao banco de dados da nuvem que foi configurado na se\u00e7\u00e3o anterior. O processo \u00e9:</p> <ol> <li>Criar uma inst\u00e2ncia espec\u00edfica para o aplicativo do mysql-router com o charm subordinado mysql-router;</li> <li>Adicionar uma rela\u00e7\u00e3o entre a inst\u00e2ncia do mysql-router e o banco de dados;</li> <li>Adicionar uma rela\u00e7\u00e3o entre a inst\u00e2ncia do mysql-router e o aplicativo.</li> </ol> <p>A combina\u00e7\u00e3o dos passos 2 e 3 conecta o aplicativo ao banco de dados da nuvem.</p> <p>Aqui est\u00e3o os comandos correspondentes para o Vault:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router vault-mysql-router\njuju integrate vault-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate vault-mysql-router:shared-db vault:shared-db\n</code></pre>"},{"location":"roteiro3/main/#desbloqueio-unseal","title":"Desbloqueio (Unseal)","text":"<p>Agora o Vault deve ser inicializado e desbloqueado. O charm vault tamb\u00e9m precisar\u00e1 ser autorizado para realizar certas tarefas. Esses passos est\u00e3o descritos na documenta\u00e7\u00e3o do charm Vault. </p> <p>Instalando o cli do Vault e configurando-o:</p> <pre><code>sudo snap install vault\nexport VAULT_ADDR=\"http://&lt;IP of vault unit&gt;:8200\"\n</code></pre> <p>Gerando:</p> <pre><code>$ vault operator init -key-shares=5 -key-threshold=3\n</code></pre> <p>Assim iremos receber 5 Unseal Keys e 1 Initial Root Token. Copie e guarde as hashs geradas. Removendo o selo, repita a opera\u00e7\u00e3o com 3 keys diferentes:</p> <p><pre><code>vault operator unseal &lt;Unseal Key&gt; # com 3 keys diferentes\n</code></pre> Autorizando o charm (esse passo precisa ser feito em 50 minutos):</p> <pre><code>export VAULT_TOKEN=&lt;Initial Root Token&gt;\nvault token create -ttl=50m\n</code></pre> <p>Anote o token gerado pelo comando e em seguida digite o comando:</p> <pre><code>juju run vault/leader authorize-charm token=token\n</code></pre>"},{"location":"roteiro3/main/#certificado-da-ca-autossinado","title":"Certificado da CA (Autossinado)","text":"<p>Para fornecer ao Vault um certificado de autoridade (CA), \u00e9 necess\u00e1rio gerar um certificado CA autossinado, para que ele possa emitir certificados para os servi\u00e7os da API da nuvem. Isso est\u00e1 descrito na p\u00e1gina Gerenciando certificados TLS. Execute isso agora:</p> <pre><code>juju run vault/leader generate-root-ca\n</code></pre> <p>Os aplicativos da nuvem s\u00e3o habilitados para TLS por meio da rela\u00e7\u00e3o vault:certificates. Abaixo, come\u00e7amos com o banco de dados da nuvem. Embora ele j\u00e1 possua um certificado autossinado, \u00e9 recomendado usar um certificado assinado pela CA do Vault:</p> <pre><code>juju integrate mysql-innodb-cluster:certificates vault:certificates\n</code></pre>"},{"location":"roteiro3/main/#neutron","title":"Neutron","text":"<p>A rede Neutron \u00e9 implementada com quatro aplicativos:</p> <ul> <li>neutron-api</li> <li>neutron-api-plugin-ovn (subordinado)</li> <li>ovn-central</li> <li>ovn-chassis (subordinado)</li> </ul> <p>O arquivo neutron.yaml cont\u00e9m as configura\u00e7\u00f5es necess\u00e1rias (apenas dois dos aplicativos requerem configura\u00e7\u00e3o):</p> <p>neutron.yaml</p> <pre><code>ovn-chassis:\n  bridge-interface-mappings: br-ex:eth0\n  ovn-bridge-mappings: physnet1:br-ex\n\nneutron-api:\n  neutron-security-groups: true\n  flat-network-providers: physnet1\n</code></pre> <p>A configura\u00e7\u00e3o <code>bridge-interface-mappings</code> impacta o OVN Chassis e se refere a um mapeamento de ponte OVS para interface de rede. Conforme descrito na se\u00e7\u00e3o Criar ponte OVS na p\u00e1gina Instalar MAAS, neste exemplo \u00e9 <code>br-ex:enp1s0</code>.</p> <p>Nota</p> <p>Para usar endere\u00e7os de hardware (em vez de nomes de interface comuns a todos os tr\u00eas n\u00f3s), a op\u00e7\u00e3o <code>bridge-interface-mappings</code> pode ser expressa da seguinte forma (substitua pelos seus pr\u00f3prios valores):</p> <p>neutron.yaml</p> <pre><code>bridge-interface-mappings: &gt;-\n  br-ex:52:54:00:03:01:02\n  br-ex:52:54:00:03:01:03\n  br-ex:52:54:00:03:01:04\n</code></pre> <p>A configura\u00e7\u00e3o <code>flat-network-providers</code> ativa o provedor de rede flat do Neutron usado neste cen\u00e1rio de exemplo e atribui a ele o nome physnet1. O provedor de rede flat e seu nome ser\u00e3o referenciados quando configurarmos a rede p\u00fablica na pr\u00f3xima p\u00e1gina.</p> <p>A configura\u00e7\u00e3o <code>ovn-bridge-mappings</code> mapeia a interface de dados para o provedor de rede flat.</p> <p>O aplicativo principal do OVN \u00e9 o ovn-central, e ele requer pelo menos tr\u00eas unidades. Elas ser\u00e3o containerizadas nas m\u00e1quinas 0, 1 e 2 com o charm ovn-central. Para implantar:</p> <pre><code>juju deploy -n 3 --to lxd:0,lxd:1,lxd:2 --channel 22.03/stable ovn-central\n</code></pre> <p>O aplicativo neutron-api ser\u00e1 containerizado na m\u00e1quina 1 com o charm neutron-api:</p> <pre><code>juju deploy --to lxd:1 --channel yoga/stable --config neutron.yaml neutron-api\n</code></pre> <p>Implante os charms subordinados com os charms neutron-api-plugin-ovn e ovn-chassis:</p> <pre><code>juju deploy --channel yoga/stable neutron-api-plugin-ovn\njuju deploy --channel 22.03/stable --config neutron.yaml ovn-chassis\n</code></pre> <p>Adicione as rela\u00e7\u00f5es necess\u00e1rias:</p> <pre><code>juju integrate neutron-api-plugin-ovn:neutron-plugin neutron-api:neutron-plugin-api-subordinate\njuju integrate neutron-api-plugin-ovn:ovsdb-cms ovn-central:ovsdb-cms\njuju integrate ovn-chassis:ovsdb ovn-central:ovsdb\njuju integrate ovn-chassis:nova-compute nova-compute:neutron-plugin\njuju integrate neutron-api:certificates vault:certificates\njuju integrate neutron-api-plugin-ovn:certificates vault:certificates\njuju integrate ovn-central:certificates vault:certificates\njuju integrate ovn-chassis:certificates vault:certificates\n</code></pre> <p>Conecte o neutron-api ao banco de dados da nuvem:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router neutron-api-mysql-router\njuju integrate neutron-api-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate neutron-api-mysql-router:shared-db neutron-api:shared-db\n</code></pre>"},{"location":"roteiro3/main/#keystone","title":"Keystone","text":"<p>O aplicativo keystone ser\u00e1 containerizado na m\u00e1quina 0 com o charm keystone. Para implantar:</p> <pre><code>juju deploy --to lxd:0 --channel yoga/stable keystone\n</code></pre> <p>Conecte o keystone ao banco de dados da nuvem:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router keystone-mysql-router\njuju integrate keystone-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate keystone-mysql-router:shared-db keystone:shared-db\n</code></pre> <p>Duas rela\u00e7\u00f5es adicionais podem ser adicionadas neste momento:</p> <pre><code>juju integrate keystone:identity-service neutron-api:identity-service\njuju integrate keystone:certificates vault:certificates\n</code></pre>"},{"location":"roteiro3/main/#rabbitmq","title":"RabbitMQ","text":"<p>O aplicativo rabbitmq-server ser\u00e1 containerizado na m\u00e1quina 2 com o charm rabbitmq-server. Para implantar:</p> <pre><code>juju deploy --to lxd:2 --channel 3.9/stable rabbitmq-server\n</code></pre> <p>Duas rela\u00e7\u00f5es podem ser adicionadas neste momento:</p> <pre><code>juju integrate rabbitmq-server:amqp neutron-api:amqp\njuju integrate rabbitmq-server:amqp nova-compute:amqp\n</code></pre>"},{"location":"roteiro3/main/#nova-cloud-controller","title":"Nova Cloud Controller","text":"<p>O aplicativo nova-cloud-controller, que inclui os servi\u00e7os nova-scheduler, nova-api e nova-conductor, ser\u00e1 containerizado na m\u00e1quina 2 com o charm nova-cloud-controller. O arquivo ncc.yaml cont\u00e9m a configura\u00e7\u00e3o:</p> <p>ncc.yaml</p> <pre><code>nova-cloud-controller:\n  network-manager: Neutron\n</code></pre> <p>Para implantar:</p> <pre><code>juju deploy --to lxd:2 --channel yoga/stable --config ncc.yaml nova-cloud-controller\n</code></pre> <p>Conecte o nova-cloud-controller ao banco de dados da nuvem:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router ncc-mysql-router\njuju integrate ncc-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate ncc-mysql-router:shared-db nova-cloud-controller:shared-db\n</code></pre> <p>Para manter a sa\u00edda do <code>juju status</code> mais compacta, o nome esperado da aplica\u00e7\u00e3o nova-cloud-controller-mysql-router foi abreviado para ncc-mysql-router.</p> <p>Cinco rela\u00e7\u00f5es adicionais podem ser adicionadas neste momento:</p> <pre><code>juju integrate nova-cloud-controller:identity-service keystone:identity-service\njuju integrate nova-cloud-controller:amqp rabbitmq-server:amqp\njuju integrate nova-cloud-controller:neutron-api neutron-api:neutron-api\njuju integrate nova-cloud-controller:cloud-compute nova-compute:cloud-compute\njuju integrate nova-cloud-controller:certificates vault:certificates\n</code></pre>"},{"location":"roteiro3/main/#placement","title":"Placement","text":"<p>O aplicativo placement ser\u00e1 containerizado na m\u00e1quina 2 com o charm placement. Para implantar:</p> <pre><code>juju deploy --to lxd:2 --channel yoga/stable placement\n</code></pre> <p>Conecte o placement ao banco de dados da nuvem:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router placement-mysql-router\njuju integrate placement-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate placement-mysql-router:shared-db placement:shared-db\n</code></pre> <p>Tr\u00eas rela\u00e7\u00f5es adicionais podem ser adicionadas neste momento:</p> <pre><code>juju integrate placement:identity-service keystone:identity-service\njuju integrate placement:placement nova-cloud-controller:placement\njuju integrate placement:certificates vault:certificates\n</code></pre>"},{"location":"roteiro3/main/#horizon-openstack-dashboard","title":"Horizon - OpenStack Dashboard","text":"<p>O aplicativo openstack-dashboard (Horizon) ser\u00e1 containerizado na m\u00e1quina 2 com o charm openstack-dashboard. Para implantar:</p> <pre><code>juju deploy --to lxd:2 --channel yoga/stable openstack-dashboard\n</code></pre> <p>Conecte o openstack-dashboard ao banco de dados da nuvem:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router dashboard-mysql-router\njuju integrate dashboard-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate dashboard-mysql-router:shared-db openstack-dashboard:shared-db\n</code></pre> <p>Para manter a sa\u00edda do <code>juju status</code> mais compacta, o nome esperado da aplica\u00e7\u00e3o openstack-dashboard-mysql-router foi abreviado para dashboard-mysql-router.</p> <p>Duas rela\u00e7\u00f5es adicionais podem ser adicionadas neste momento:</p> <pre><code>juju integrate openstack-dashboard:identity-service keystone:identity-service\njuju integrate openstack-dashboard:certificates vault:certificates\n</code></pre>"},{"location":"roteiro3/main/#glance","title":"Glance","text":"<p>O aplicativo glance ser\u00e1 containerizado na m\u00e1quina 2 com o charm glance. Para implantar:</p> <pre><code>juju deploy --to lxd:2 --channel yoga/stable glance\n</code></pre> <p>Conecte o glance ao banco de dados da nuvem:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router glance-mysql-router\njuju integrate glance-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate glance-mysql-router:shared-db glance:shared-db\n</code></pre> <p>Quatro rela\u00e7\u00f5es adicionais podem ser adicionadas neste momento:</p> <pre><code>juju integrate glance:image-service nova-cloud-controller:image-service\njuju integrate glance:image-service nova-compute:image-service\njuju integrate glance:identity-service keystone:identity-service\njuju integrate glance:certificates vault:certificates\n</code></pre>"},{"location":"roteiro3/main/#ceph-monitor","title":"Ceph Monitor","text":"<p>O aplicativo ceph-mon ser\u00e1 containerizado nas m\u00e1quinas 0, 1 e 2 com o charm ceph-mon. O arquivo ceph-mon.yaml cont\u00e9m a configura\u00e7\u00e3o:</p> <p>ceph-mon.yaml</p> <pre><code>ceph-mon:\n  expected-osd-count: 3\n  monitor-count: 3\n</code></pre> <p>A configura\u00e7\u00e3o acima informa ao cluster MON que ele \u00e9 composto por tr\u00eas n\u00f3s e que deve esperar pelo menos tr\u00eas OSDs (discos).</p> <p>Para implantar:</p> <pre><code>juju deploy -n 3 --to lxd:0,lxd:1,lxd:2 --channel quincy/stable --config ceph-mon.yaml ceph-mon\n</code></pre> <p>Tr\u00eas rela\u00e7\u00f5es podem ser adicionadas neste momento:</p> <pre><code>juju integrate ceph-mon:osd ceph-osd:mon\njuju integrate ceph-mon:client nova-compute:ceph\njuju integrate ceph-mon:client glance:ceph\n</code></pre> <p>Sobre as rela\u00e7\u00f5es acima:</p> <ul> <li> <p>A rela\u00e7\u00e3o nova-compute:ceph faz com que o Ceph seja o backend de armazenamento para as imagens de disco n\u00e3o boot\u00e1veis do Nova. Para que isso tenha efeito, a op\u00e7\u00e3o do charm nova-compute, chamada <code>libvirt-image-backend</code>, deve estar definida como <code>'rbd'</code>.</p> </li> <li> <p>A rela\u00e7\u00e3o glance:ceph faz com que o Ceph seja o backend de armazenamento para o Glance.</p> </li> </ul>"},{"location":"roteiro3/main/#cinder","title":"Cinder","text":"<p>O aplicativo cinder ser\u00e1 containerizado na m\u00e1quina 1 com o charm cinder. O arquivo cinder.yaml cont\u00e9m a seguinte configura\u00e7\u00e3o:</p> <p>cinder.yaml</p> <pre><code>cinder:\n  block-device: None\n  glance-api-version: 2\n</code></pre> <p>A op\u00e7\u00e3o <code>block-device</code> est\u00e1 definida como <code>'None'</code> para indicar que o charm n\u00e3o deve gerenciar dispositivos de bloco. A op\u00e7\u00e3o <code>glance-api-version</code> est\u00e1 definida como <code>'2'</code> para indicar que deve ser usada a vers\u00e3o 2 da API do Glance.</p> <p>Para implantar:</p> <pre><code>juju deploy --to lxd:1 --channel yoga/stable --config cinder.yaml cinder\n</code></pre> <p>Conecte o cinder ao banco de dados da nuvem:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router cinder-mysql-router\njuju integrate cinder-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate cinder-mysql-router:shared-db cinder:shared-db\n</code></pre> <p>Cinco rela\u00e7\u00f5es adicionais podem ser adicionadas neste momento:</p> <pre><code>juju integrate cinder:cinder-volume-service nova-cloud-controller:cinder-volume-service\njuju integrate cinder:identity-service keystone:identity-service\njuju integrate cinder:amqp rabbitmq-server:amqp\njuju integrate cinder:image-service glance:image-service\njuju integrate cinder:certificates vault:certificates\n</code></pre> <p>A rela\u00e7\u00e3o acima com glance:image-service permitir\u00e1 que o Cinder consuma a API do Glance (por exemplo, possibilitando ao Cinder realizar snapshots de volumes a partir de imagens do Glance).</p> <p>Assim como o Glance, o Cinder utilizar\u00e1 o Ceph como backend de armazenamento (da\u00ed o uso de <code>block-device: None</code> no arquivo de configura\u00e7\u00e3o). Isso ser\u00e1 implementado por meio do charm subordinado cinder-ceph:</p> <pre><code>juju deploy --channel yoga/stable cinder-ceph\n</code></pre> <p>Tr\u00eas rela\u00e7\u00f5es podem ser adicionadas neste momento:</p> <pre><code>juju integrate cinder-ceph:storage-backend cinder:storage-backend\njuju integrate cinder-ceph:ceph ceph-mon:client\njuju integrate cinder-ceph:ceph-access nova-compute:ceph-access\n</code></pre>"},{"location":"roteiro3/main/#ceph-rados-gateway","title":"Ceph RADOS Gateway","text":"<p>O Ceph RADOS Gateway ser\u00e1 implantado para oferecer uma interface HTTP compat\u00edvel com S3 e Swift. Essa \u00e9 uma alternativa ao uso do OpenStack Swift.</p> <p>O aplicativo ceph-radosgw ser\u00e1 containerizado na m\u00e1quina 0 com o charm ceph-radosgw. Para implantar:</p> <pre><code>juju deploy --to lxd:0 --channel quincy/stable ceph-radosgw\n</code></pre> <p>Uma \u00fanica rela\u00e7\u00e3o \u00e9 necess\u00e1ria:</p> <pre><code>juju integrate ceph-radosgw:mon ceph-mon:radosgw\n</code></pre>"},{"location":"roteiro3/main/#ceph-osd-integration","title":"Ceph-OSD Integration","text":"<p>Por fim, chegamos ao \u00faltimo passo para terminar e integrar todas as nossas ferramentas. Execute o comando abaixo apenas se voc\u00ea estiver certeza de que todos os procedimentos anteriores foram realizados com sucesso.</p> <pre><code>juju config ceph-osd osd-devices='/dev/sdb'\n</code></pre> <p>Ap\u00f3s esse \u00faltimo passo, voc\u00ea pode executar o comando juju status e seu terminal deve parecer algo com isso:</p> <p></p>"},{"location":"roteiro3/main/#setup","title":"Setup","text":"<p>Agora vamos configurar os servi\u00e7os das VMs, dos discos e da estrutura de rede virtual</p> <p>Para isso, \u00e9 preciso seguir alguns passos:</p> <ul> <li>carregar as vari\u00e1veis de ambiente e se autenticar no sistema atrav\u00e9s do openrc;</li> <li>utilizar o dashboard para visualizar as mudan\u00e7as;</li> <li>importar uma imagem do Ubuntu Jammy;</li> <li>criar os flavors para as VMs;</li> <li>criar uma rede externa para conectar as VMs \u00e0 rede f\u00edsica;</li> <li>criar uma rede interna e um roteador para conectar as VMs \u00e0 rede externa.</li> </ul> <p>Para obter controle da nuvem, a senha de administrador do Keystone e o certificado da CA s\u00e3o necess\u00e1rios. Essas informa\u00e7\u00f5es podem ser obtidas mais facilmente usando um arquivo criado para esse fim \u2014 frequentemente chamado de arquivo \u201copenrc\u201d.</p> <p>Baixe o arquivo openrc no link e guarde-o em um local seguro.</p> <p>Quando quiser usar esse arquivo, basta excutar ele. Supondo que esteja agora no seu diret\u00f3rio principal:</p> <pre><code>source openrc\n</code></pre>"},{"location":"roteiro3/main/#acessando-o-dashborad-horizon","title":"Acessando o dashborad Horizon","text":"<p>Podemos acessar o dashboard do Horizon utilizando um t\u00fanel. Para isso precisamos saber o IP do dashboard, qual porta est\u00e1 escutando e tamb\u00e9m as credenciais de login. Vamos primeiro obter o IP do dashboard:</p> <pre><code>juju status\n</code></pre> <p></p> <p>Obtemos ent\u00e3o o IP e a porta atrav\u00e9s do juju status. Vamos agora obter a senha de admin, utilizando o seguinte comando no openstack:</p> <pre><code>juju exec --unit keystone/leader leader-get admin_passwd\n</code></pre> <p>Com isso as credenciais de admin do horizon ser\u00e3o:</p> <ul> <li>User name: admin</li> <li>Password: senha do \u00faltimo comando</li> <li>Domain: admin_domain</li> </ul> <p>Agora vamos fazer o t\u00fanel para acessar o dashboard:</p> <pre><code>ssh cloud@IP_MAIN -L 8080:IP_DO_DASHBOARD:80\n</code></pre> <p>Aqui tanto faz a porta ser 8080 ou 8081 etc, mas a porta 80 \u00e9 a porta do dashboard horizon que obtemos do juju status e n\u00e3o pode ser outra porta al\u00e9m dessa</p> <p>Agora podemos acessar no navegador atrav\u00e9s do link http://IP_DASHBOARD/horizon e fazer login e assim conseguimos visualizar todo o ambiente do Horizon:</p> <p>OVERVIEW </p> <p>INSTANCES </p> <p>TOPOLOGY </p> <p>MAAS </p>"},{"location":"roteiro3/main/#images-e-flavors","title":"Images e flavors","text":"<p>Agora vamos trabalhar com images e flavors, mas antes vamos carregar as credenciais:</p> <p><pre><code>source openrc\n</code></pre> Antes pequenos ajustes na rede:</p> <pre><code>juju config neutron-api enable-ml2-dns=\"true\"\njuju config neutron-api-plugin-ovn dns-servers=\"172.16.0.1\"\n</code></pre> <p>Importe uma imagem de boot no Glance para criar inst\u00e2ncias de servidor. Aqui, importamos uma imagem Jammy amd64:</p> <pre><code>mkdir ~/cloud-images\n\nwget http://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img \\\n   -O ~/cloud-images/jammy-amd64.img\n</code></pre> <pre><code>openstack image create --public --container-format bare \\\n   --disk-format qcow2 --file ~/cloud-images/jammy-amd64.img \\\n   jammy-amd64\n</code></pre> <p>Crie os flavors (instance type) - SEM ephemeral disk:</p> Flavor Name vCPUs RAM (GB) Disk m1.tiny 1 1 20 m1.small 1 2 20 m1.medium 2 4 20 m1.large 4 8 20"},{"location":"roteiro3/main/#rede-externa","title":"Rede externa","text":"<p>Crie uma rede externa p\u00fablica (compartilhada), aqui chamada de \u2018ext_net\u2019. Usamos o tipo de provedor de rede \u2018flat\u2019 e seu provedor \u2018physnet1\u2019:</p> <pre><code>openstack network create --external --share \\\n   --provider-network-type flat --provider-physical-network physnet1 \\\n   ext_net\n</code></pre> <p>Crie a sub-rede, aqui chamada de \u2018ext_subnet\u2019, para a rede acima. Os valores utilizados s\u00e3o baseados no ambiente local. Usar uma faixa de aloca\u00e7\u00e3o entre 172.16.7.0 e 172.16.8.255:</p> <pre><code>openstack subnet create --network ext_net --no-dhcp \\\n   --gateway 172.16.0.1 --subnet-range 172.16.0.0/20 \\\n   --allocation-pool start=172.16.7.0,end=172.16.8.255 \\\n   ext_subnet\n</code></pre>"},{"location":"roteiro3/main/#rede-interna-e-roteador","title":"Rede interna e roteador","text":"<p>Para obter um endere\u00e7o IP fixo para acessar quaisquer inst\u00e2ncias criadas, precisamos de uma rede espec\u00edfica do projeto com uma sub-rede privada. Tamb\u00e9m precisaremos de um roteador para conectar essa rede \u00e0 rede p\u00fablica criada anteriormente.</p> <p>O usu\u00e1rio n\u00e3o-administrador agora cria uma rede interna privada chamada \u2018user1_net\u2019 e uma sub-rede correspondente chamada \u2018user1_subnet\u2019. Usar a subnet 192.169.0.0/24. N\u00e3o use DNS:</p> <pre><code>openstack network create --internal user1_net\n\nopenstack subnet create --network user1_net \\\n   --subnet-range 192.169.0.0/24 \\\n   --allocation-pool start=192.169.0.10,end=192.169.0.99 \\\n   user1_subnet\n</code></pre> <p>Agora, um roteador chamado \u2018user1_router\u2019 \u00e9 criado, adicionado \u00e0 sub-rede e configurado para usar a rede externa p\u00fablica como sua rede de gateway:</p> <pre><code>openstack router create user1_router\nopenstack router add subnet user1_router user1_subnet\nopenstack router set user1_router --external-gateway ext_net\n</code></pre>"},{"location":"roteiro3/main/#conexao","title":"Conex\u00e3o","text":"<p>Um par de chaves SSH precisa ser importado para a nuvem a fim de acessar suas inst\u00e2ncias. Gere um primeiro, caso ainda n\u00e3o tenha. Este comando cria um par de chaves sem senha (remova a op\u00e7\u00e3o -N para evitar isso):</p> <pre><code>mkdir ~/cloud-keys\n\nssh-keygen -q -N '' -f ~/cloud-keys/user1-key\n</code></pre> <pre><code>openstack keypair create --public-key ~/cloud-keys/user1-key.pub user1\n</code></pre> <p>Grupos de seguran\u00e7a precisar\u00e3o ser configurados para permitir o tr\u00e1fego SSH. Voc\u00ea pode alterar as regras do grupo padr\u00e3o ou criar um novo grupo com suas pr\u00f3prias regras. Faremos o \u00faltimo, criando um grupo chamado \u2018Allow_SSH\u2019:</p> <pre><code>openstack security group create --description 'Allow SSH' Allow_SSH\nopenstack security group rule create --proto tcp --dst-port 22 Allow_SSH\n</code></pre>"},{"location":"roteiro3/main/#instancia","title":"Inst\u00e2ncia","text":"<p>Agora vamos criar uma inst\u00e2ncia com imagem jammy amd64 e com flavor m1.tiny de nome client e sem Novo Volume:</p> <pre><code>openstack server create --image jammy-amd64 --flavor m1.tiny \\\n   --key-name user1 --network user1_net --security-group Allow_SSH \\\n   client\n</code></pre> <p>Solicite e atribua um endere\u00e7o IP flutuante \u00e0 nova inst\u00e2ncia:</p> <pre><code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)\nopenstack server add floating ip client $FLOATING_IP\n</code></pre> <p>Agora vamos conectar com a inst\u00e2ncia e testar se tudo funciona:</p> <pre><code>ssh -i ~/cloud-keys/user1-key ubuntu@$FLOATING_IP\n</code></pre> <p>Se o terminal da inst\u00e2ncia for acessado com sucesso ent\u00e3o a inst\u00e2ncia est\u00e1 corretamente configurada.</p> <p>OVERVIEW </p> <p>INSTANCES </p> <p>TOPOLOGY </p> <p>MAAS </p> <p>Agora \u00e9 poss\u00edvel observar atrav\u00e9s do Horizon que ap\u00f3s todos esses passos:</p> <ul> <li>Overview: Mostra pequenos gr\u00e1ficos em pizza com a quantidade de inst\u00e2ncias criadas, IP flutuante, grupo de seguran\u00e7a, redes etc.</li> <li>Instances: Mostra a nossa inst\u00e2ncia client criada.</li> <li>Topology: Mostra as nossas duas redes externa e interna</li> </ul>"},{"location":"roteiro3/main/#app","title":"App","text":"<p>Agora vamos criar nossa infraestrutura, e para isso vamos cumprir umas tarefas:</p> <p>1) Criar 2 inst\u00e2ncias com API do projeto, etapa 1 2) Criar 1 inst\u00e2ncia com banco de dados, etapa 1 3) Criar 1 inst\u00e2ncia com LoadBalancer, nginx</p> <p>Iremos configurar seguindo a topologia:</p> <pre><code>flowchart LR\n    subgraph private [192.169.0.0/24]\n        direction TB\n        lb e2@==&gt; api1[API]\n        lb e3@==&gt; api2[API]\n        api1 e4@==&gt; db\n        api2 e5@==&gt; db\n    end\n    user e1@==&gt;|request&lt;br&gt;172.16.0.0/20| lb\n    e1@{ animate: true }\n    e2@{ animate: true }\n    e3@{ animate: true }\n    e4@{ animate: true }\n    e5@{ animate: true }\n    lb@{ shape: div-rect, label: \"Load Balancer\" }\n    db@{ shape: cyl, label: \"Database\" }\n    user@{ img: \"https://insper.github.io/computacao-nuvem/assets/images/fontawesome-user-icon.png\", constraint: \"on\", h: 60, label: \"User\" }</code></pre> <p>Vamos subir todas as inst\u00e2ncias primeiro e em seguida configurar cada uma delas:</p> <p></p>"},{"location":"roteiro3/main/#tarefa-1","title":"Tarefa 1","text":"<p>Para criar as inst\u00e2ncias da API precisamos pegar a imagem que est\u00e1 no docker Hub com a API que criamos na etapa 1 do projeto. Mas antes precisamos instalar a engine do docker via terminal em TODAS inst\u00e2ncias exceto do nginx:</p> <pre><code>sudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n</code></pre> <p>Agora vamos baixar a imagem do docker hub:</p> <pre><code>sudo docker run -d -p 8080:8080 --env-file .env seu_username/nome-da-imagem\n</code></pre> <p>Certifique de ter um arquivo .env com as credenciais do database que contenha o usuario, nome do database, link do host etc.</p> <p>Fazemos esses 2 passos para ambas as inst\u00e2ncias que ir\u00e3o conter a nossa API. Para verificar se est\u00e1 tudo ok com nossa API execute o comando:</p> <pre><code>sudo docker ps -a\n</code></pre>"},{"location":"roteiro3/main/#tarefa-2","title":"Tarefa 2","text":"<p>Para a tarefa 2 vamos instalar e utilizar o postgres:</p> <pre><code>sudo apt update\nsudo apt install postgresql postgresql-contrib -y\n</code></pre> <p>Em seguida vamos acessar o console do postgres:</p> <pre><code>sudo -u postgres psql\n</code></pre> <p>Voc\u00ea pode definir uma senha nova:</p> <pre><code>\\password senha\n</code></pre> <p>Digite duas vezes</p> <p>Agora criamos o database com usuario garantindo privil\u00e9gios:</p> <pre><code>CREATE DATABASE database;\nCREATE USER app_user WITH PASSWORD 'sua_senha';\nGRANT ALL PRIVILEGES ON DATABASE database TO fastapi_app;\n\\q  # Para sair\n</code></pre> <p>Agora vamos editar o arquivo de configura\u00e7\u00e3o do postgres:</p> <pre><code>sudo nano /etc/postgresql/*/main/postgresql.conf\n</code></pre> <p>Aqui localize e modifique a linha para:</p> <pre><code>listen_addresses = '*' # para permitir todos\n</code></pre> <p>Depois vamos editar o arquivo de conex\u00e3o:</p> <pre><code>sudo nano /etc/postgresql/*/main/pg_hba.conf\n</code></pre> <p>Adicione no final:</p> <pre><code>host all all 172.16.0.0/20 trust\nhost all all 192.169.0.82/32 md5\nhost all all 192.169.0.48/32 md5\n</code></pre> <p>E reinicie o postgres:</p> <pre><code>sudo systemctl restart postgresql\n</code></pre> <p>Agora liberando o firewall:</p> <pre><code>sudo ufw allow 5432/tcp\nsudo ufw reload\n</code></pre> <p>E agora nosso database est\u00e1 pronto. Lembre-se das credenciais do database, como usuario, senha, link do host etc., que dever\u00e3o ser colocados em um arquivo .env nas inst\u00e2ncias da API.</p>"},{"location":"roteiro3/main/#tarefa-3","title":"Tarefa 3","text":"<p>Para configurar nosso loadbalencer vamos primeiro instalar:</p> <pre><code>sudo apt-get install nginx\n</code></pre> <p>Agora vamos configuar o nginx. Para configurar um loadbalancer round robin, precisaremos usar o m\u00f3dulo upstream do nginx. Incorporaremos a configura\u00e7\u00e3o nas defini\u00e7\u00f5es do nginx:</p> <pre><code>sudo nano /etc/nginx/sites-available/default\n</code></pre> <p>Precisamos incluir o m\u00f3dulo upstream, que se parece com isto:</p> <pre><code>upstream backend { server link_api_1; server link_api_2; }\n</code></pre> <p>Depois, devemos referenciar o m\u00f3dulo mais adiante na configura\u00e7\u00e3o:</p> <pre><code>server { location / { proxy_pass http://backend; } }\n</code></pre> <p>Por fim reinicie o nginx:</p> <pre><code>sudo service nginx restart\n</code></pre> <p>Com isso nosso nginx est\u00e1 pronto e ir\u00e1 distribuir entre as APIs.</p>"},{"location":"roteiro3/main/#testando","title":"Testando","text":"<p>Para testar se nossas APIs funcionam corretamente nas inst\u00e2ncias criadas podemos utilizar um t\u00fanel com o IP externo do nginx que por sua vez ir\u00e1 redirecionar para uma das APIs:</p> <pre><code>ssh cloud@IP_MAIN -L 8080:IP_DO_NGINX:PORTA_NGINX\n</code></pre> <p>A porta nginx \u00e9 a porta definida dentro do arquivo de configura\u00e7\u00e3o no nginx</p> <p>Se o dashboard do docs da API aparecer significa que est\u00e1 tudo funcionando corretamente.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"roteiro4/main/","title":"Objetivos","text":"<ol> <li>Entender os conceitos b\u00e1sicos Infraestrutura como c\u00f3digo.</li> <li>Entender os conceitos b\u00e1sicos sobre SLA e DR.</li> </ol>"},{"location":"roteiro4/main/#infra","title":"Infra","text":"<p>A infraestrutura necess\u00e1ria para cumprir com os objetivos desse roteiro consiste em segmentar por aluno - de forma a criar uma separa\u00e7\u00e3o l\u00f3gica entre os recursos utilizados por estes - e estruturar uma hierarquia de projeto para cada atrav\u00e9s do dashboard do OpenStack. Para isso, seguimos o seguinte passo-a-passo:</p>"},{"location":"roteiro4/main/#1-criacao-de-um-domain-unico","title":"1. Cria\u00e7\u00e3o de um Domain \u00fanico","text":"<p>No dashboard do OpenStack, nevagamos at\u00e9 Identity &gt; Domains, clicamos em \"Create Domain\" e adicionamos o dom\u00ednio AlunosDomain. Em seguida, definimos AlunosDomain como o novo contexto de uso.</p>"},{"location":"roteiro4/main/#2-criacao-de-um-projeto-para-cada-aluno","title":"2. Cria\u00e7\u00e3o de um projeto para cada aluno","text":"<p>Em Identity &gt; Projects, criamos os projetos <code>KitV_Vinicius</code> e <code>KitV_AndersonFranco</code>. Em seguida, em Identity &gt; Users, criamos os usu\u00e1rios Vinicius, com papel administrativo no projeto KitV_Vinicius, e AndersonFranco, com papel administrativo no projeto KitV_AndersonFranco.</p>"},{"location":"roteiro4/main/#setup","title":"Setup","text":""},{"location":"roteiro4/main/#criando-a-infraestrutura-utilizando-iac","title":"Criando a Infraestrutura utilizando IaC","text":"<p>Para criar a infraestrutura usando IaC, utilizamos o Terraform. O Terraform conta com arquivos .tf, que definem a infraestrutura.</p> <p>O Terraform sabe o que \u00e9 implantado por meio do arquivo de estado. Este estado \u00e9 armazenado por padr\u00e3o em um arquivo local chamado \u201cterraform.tfstate\u201d. Tamb\u00e9m pode ser armazenado remotamente, o que funciona melhor em um ambiente de equipe.</p> <p>O Terraform permite v\u00e1rios provedores, normalmente inicializados em um arquivo chamado provider.tf.</p> <p>Em cada projeto criado anteriormente (KitV_Vinicius e KitV_AndersonFranco), criamos a pasta terraform, com a seguinte estrutura de arquivos:</p> <p></p> <p>Fonte: Roteiro 4 - Cloud</p> <p>Cada um desses arquivos foi preenchido com os templates a seguir e posteriormente modificados com as informa\u00e7\u00f5es: username, key pair e external network id.</p> <p>Templates:</p> <p><code>provider.tf</code></p> <pre><code># Terraform Openstack deployment\n# Author: Tiago Demay - tiagoaodc@insper.edu.br\n\n\n# Define required providers\nterraform {\nrequired_version = \"&gt;= 0.14.0\"\n  required_providers {\n    openstack = {\n      source  = \"terraform-provider-openstack/openstack\"\n      version = \"~&gt; 1.35.0\"\n    }\n  }\n}\n\n\n# Configure the OpenStack Provider\n\nprovider \"openstack\" {\n  region              = \"RegionOne\"\n  user_name           = \"SEU_USUARIO\"\n}\n</code></pre> <p><code>instance1.tf</code></p> <pre><code>resource \"openstack_compute_instance_v2\" \"instancia\" {\n  name            = \"basic\"\n  image_name      = \"bionic-amd64\"\n  flavor_name     = \"m1.small\"\n  key_pair        = \"mykey\"\n  security_groups = [\"default\"]\n\n  network {\n    name = \"network_1\"\n  }\n\n  depends_on = [openstack_networking_network_v2.network_1]\n\n}\n</code></pre> <p><code>instance2.tf</code></p> <pre><code>resource \"openstack_compute_instance_v2\" \"instancia\" {\n  name            = \"basic2\"\n  image_name      = \"jammy-amd64\"\n  flavor_name     = \"m1.tiny\"\n  key_pair        = \"mykey\"\n  security_groups = [\"default\"]\n\n  network {\n    name = \"network_1\"\n  }\n\n  depends_on = [openstack_networking_network_v2.network_1]\n\n}\n</code></pre> <p><code>network.tf</code></p> <pre><code>resource \"openstack_networking_network_v2\" \"network_1\" {\n  name           = \"network_1\"\n  admin_state_up = \"true\"\n}\n\nresource \"openstack_networking_subnet_v2\" \"subnet_1\" {\n  network_id = \"${openstack_networking_network_v2.network_1.id}\"\n  cidr       = \"192.167.199.0/24\"\n}\n</code></pre> <p><code>router.tf</code></p> <pre><code>resource \"openstack_networking_router_v2\" \"router_1\" {\n  name                = \"my_router\"\n  admin_state_up      = true\n  external_network_id = &lt;\"ID_EXT_NETWORK\"&gt;\n}\n\nresource \"openstack_networking_router_interface_v2\" \"int_1\" {\n  router_id = \"${openstack_networking_router_v2.router_1.id}\"\n  subnet_id = \"${openstack_networking_subnet_v2.subnet_1.id}\"\n}\n</code></pre>"},{"location":"roteiro4/main/#credenciais-do-usuario","title":"Credenciais do usu\u00e1rio","text":"<p>Para poder carregar as vari\u00e1veis do ambiente de cada usu\u00e1rio, seguimos o seguinte procedimento:</p> <ol> <li>Entramos no dashboard do OpenStack com o usu\u00e1rio de cada aluno, no dom\u00ednio AlunosDomain;</li> <li>Na aba project \u2192 API access, fizemos o download do openstack rc file de cada usu\u00e1rio;</li> <li>Copiamos o conteudo em cada pasta de projeto, num arquivo openrc.sh;</li> <li>Cedemos a permiss\u00e3o de execu\u00e7\u00e3o para o arquivo <code>chmod +x arquivo.sh</code></li> <li>Carregamos as vari\u00e1veis de ambiente do administrador executando o comando <code>source openstack-base/openrc</code> na raiz do projeto;</li> <li>Carregamos as vari\u00e1veis de ambiente do nosso projeto executando o comando <code>source openrc.sh</code> dentro da pasta do projeto.</li> </ol>"},{"location":"roteiro4/main/#implementacao-da-infraestrutura","title":"Implementa\u00e7\u00e3o da infraestrutura","text":"<p>Para implementar a infraestrutura, enfim, rodamos os comandos:</p> <pre><code>terraform plan\nterraform apply\n</code></pre> <p>E atrav\u00e9s do OpenStack, verificamos a cria\u00e7\u00e3o das redes.</p>"},{"location":"roteiro4/main/#checkpoint-geral","title":"Checkpoint - Geral","text":"<p>Aba Identy &gt; projects no OpenStack.</p> <p></p> <p>Aba Identy &gt; users no OpenStack.</p> <p></p>"},{"location":"roteiro4/main/#checkpoint-vinicius","title":"Checkpoint - Vin\u00edcius","text":"<p>Aba Compute &gt; Overview no OpenStack.</p> <p></p> <p>Aba compute &gt; instances no OpenStack.</p> <p></p> <p>Aba network &gt; topology no OpenStack.</p> <p></p>"},{"location":"roteiro4/main/#checkpoint-anderson-franco","title":"Checkpoint - Anderson Franco","text":"<p>Aba Compute &gt; Overview no OpenStack.</p> <p></p> <p>Aba compute &gt; instances no OpenStack.</p> <p></p> <p>Aba network &gt; topology no OpenStack.</p> <p></p>"},{"location":"roteiro4/main/#criando-um-plano-de-disaster-recovery-e-sla","title":"Criando um plano de Disaster Recovery e SLA","text":"<p>a. Escolha da Infraestrutura</p> <p>Para este cen\u00e1rio, a Nuvem Privada \u00e9 a escolha mais prudente. A natureza sigilosa dos dados operacionais \u00e9 o fator primordial, e uma nuvem privada oferece o controle granular sobre seguran\u00e7a, governan\u00e7a e soberania de dados indispens\u00e1vel. Embora possa haver um investimento inicial, o controle sobre a infraestrutura e a previsibilidade de custos a longo prazo para uma carga de trabalho cr\u00edtica e est\u00e1vel podem resultar em um TCO (Custo Total de Propriedade) vantajoso. Este ambiente dedicado mitiga riscos associados ao compartilhamento de recursos de nuvens p\u00fablicas, essencial para dados sens\u00edveis.</p> <p>b. A Necessidade Estrat\u00e9gica de um Time de DevOps</p> <p>A constitui\u00e7\u00e3o de um time de DevOps \u00e9 fundamental para o sucesso deste projeto. DevOps, como cultura e conjunto de pr\u00e1ticas, une Desenvolvimento (Dev) e Opera\u00e7\u00f5es (Ops), promovendo agilidade e velocidade na entrega de novas funcionalidades e corre\u00e7\u00f5es atrav\u00e9s da automa\u00e7\u00e3o (CI/CD). Isso se traduz em maior qualidade e confiabilidade do sistema, com testes integrados e monitoramento cont\u00ednuo. A colabora\u00e7\u00e3o e comunica\u00e7\u00e3o otimizadas entre equipes resultam em resolu\u00e7\u00e3o de problemas mais eficiente e redu\u00e7\u00e3o de custos operacionais. Crucialmente, a abordagem DevSecOps integra a seguran\u00e7a em todo o ciclo de vida do sistema, vital para a prote\u00e7\u00e3o dos nossos dados sigilosos.</p> <p>c. Plano de Resili\u00eancia: Disaster Recovery (DR) e Alta Disponibilidade (HA)</p> <ul> <li> <p>Principais Amea\u00e7as Mapeadas: Consideramos desastres naturais, falhas de energia prolongadas, falhas de hardware e software, erros humanos, ataques cibern\u00e9ticos (ransomware, DDoS) e falhas de comunica\u00e7\u00e3o.</p> </li> <li> <p>A\u00e7\u00f5es Priorizadas para Recupera\u00e7\u00e3o (DR): Em caso de desastre, o plano prev\u00ea:</p> <ol> <li>Ativa\u00e7\u00e3o imediata da equipe de resposta e comunica\u00e7\u00e3o \u00e0s partes interessadas.</li> <li>Avalia\u00e7\u00e3o r\u00e1pida do impacto.</li> <li>Acionamento do site de DR (geograficamente distinto), com RPO (Recovery Point Objective) de minutos e RTO (Recovery Time Objective) de horas.</li> <li>Restaura\u00e7\u00e3o e valida\u00e7\u00e3o dos dados a partir dos backups mais recentes.</li> <li>Valida\u00e7\u00e3o dos sistemas e redirecionamento dos usu\u00e1rios.</li> <li>Monitoramento cont\u00ednuo e, posteriormente, planejamento do retorno ao site prim\u00e1rio (failback).</li> <li>An\u00e1lise p\u00f3s-incidente para melhoria cont\u00ednua.</li> </ol> </li> <li> <p>Pol\u00edtica de Backup: Implementaremos backups completos semanais, incrementais/diferenciais di\u00e1rios e de logs de transa\u00e7\u00e3o frequentes. A reten\u00e7\u00e3o variar\u00e1 de semanas a anos, conforme a criticidade e conformidade. C\u00f3pias ser\u00e3o armazenadas off-site (no site de DR), com criptografia e testes de restaura\u00e7\u00e3o trimestrais para garantir a integridade.</p> </li> <li> <p>Implementa\u00e7\u00e3o de Alta Disponibilidade (HA): A HA ser\u00e1 alcan\u00e7ada atrav\u00e9s de:</p> <ul> <li>Redund\u00e2ncia em todas as camadas: servidores em cluster, storage com RAID e controladoras duplas, equipamentos de rede redundantes e fontes de energia duplicadas (UPS e geradores).</li> <li>Balanceamento de carga para distribuir o tr\u00e1fego e evitar pontos \u00fanicos de falha.</li> <li>Virtualiza\u00e7\u00e3o para permitir migra\u00e7\u00e3o r\u00e1pida de VMs.</li> <li>Clusters de banco de dados com replica\u00e7\u00e3o e failover autom\u00e1tico.</li> <li>Monitoramento proativo para detec\u00e7\u00e3o antecipada de problemas.</li> </ul> </li> </ul>"}]}