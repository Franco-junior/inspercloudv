{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-v","title":"KIT-V","text":"<p>Anderson Franco</p> <p>Vin\u00edcius Rodrigues</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<ul> <li>Entender os conceitos b\u00e1sicos sobre uma plataforma de gerenciamento de hardware.</li> <li>Introduzir conceitos b\u00e1sicos sobre redes de computadores.</li> </ul>"},{"location":"roteiro1/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":""},{"location":"roteiro1/main/#tarefa-1-instalar-o-maas","title":"Tarefa 1: Instalar o MaaS","text":"<p>O MaaS um software de c\u00f3digo aberto e suportado pela Canonical. O MAAS trata servidores f\u00edsicos como m\u00e1quinas virtuais ou inst\u00e2ncias na nuvem.  Esse software possui diversas vers\u00f5es, mas para esse projeto iremos utilizar a vers\u00e3o stable 3.5.3.</p> <p>Primeiro atualizamos os pacotes do sistema:</p> <p><code>sudo apt update &amp;&amp; sudo apt upgrade -y</code></p> <p>Em seguida fazemos a instala\u00e7\u00e3o:</p> <pre><code>sudo snap install maas --channel=3.5/stable\nsudo snap install maas-test-db\n</code></pre>"},{"location":"roteiro1/main/#tarefa-2-acessando-e-configurando-o-maas","title":"Tarefa 2: Acessando e configurando o MaaS","text":"<p>Ainda conectado a m\u00e1quina local pelo cabo Ethernet podemos acessar a m\u00e1quina main atrav\u00e9s da nossa m\u00e1quina pessoal utilizando o SSH:</p> <p><pre><code>ssh cloud@172.16.0.3\n</code></pre> </p> <ul> <li>Inicializando o MaaS</li> </ul> <pre><code>sudo maas init region+rack --maas-url http://172.16.0.3:5240/MAAS --database-uri maas-test-db:///\nsudo maas createadmin\n</code></pre> <p>Nessa parte devemos criar um login e uma senha. Pelo padr\u00e3o da disciplina iremos criar o admin como \"cloud\" e a senha \"cloud\"+\"letra do Kit\", neste caso \"cloudv\"</p> <ul> <li>Gerando o par de chaves para autentica\u00e7\u00e3o</li> </ul> <p>Em seguida, precisamos gerar as chaves para autenticar atrav\u00e9s do comando:</p> <pre><code>ssh-keygen -t rsa\n</code></pre> <p></p> <p>Gerando a chave, devemos copiar e guardar em uma pasta: </p> <pre><code>cat ./.ssh/id_rsa.pub\n</code></pre> <ul> <li>Acessando o dashboard do MaaS</li> </ul> <p>Ap\u00f3s esses passos \u00e9 poss\u00edvel acessar o dashboard do MaaS para dar sequ\u00eancia as outras configura\u00e7\u00f5es necess\u00e1rias atrav\u00e9s do link http://172.16.0.3:5240/MAAS</p> <p>Agora precisamos configurar o DNS forwarder com o DNS do Insper. Para isso precisamos acessar a aba Settings/Network/DNS do dashboard:</p> <p></p> <p>Agora vamos importar as imagens do Ubuntu 22 e 24 no dashboard:</p> <p></p> <p>Em seguida devemos definir o Global Kernel Parameters:</p> <p></p>"},{"location":"roteiro1/main/#tarefa-3-chaveando-o-dhcp","title":"Tarefa 3: Chaveando o DHCP","text":"<p>Nessa etapa precisamos habilitar o DHCP no MaaS Controller, definindo o range de IPs iniciar em 172.16.11.1 e acabar em 172.16.14.255. Al\u00e9m disso, tem que deixar o DNS da subnet apontando para o DNS do Insper.</p> <p> </p>"},{"location":"roteiro1/main/#tarefa-4-checando-a-saude-do-maas","title":"Tarefa 4: Checando a sa\u00fade do Maas","text":"<p>Dentro do dashboard, na aba de Controladores, \u00e9 poss\u00edvel checar a \"sa\u00fade\" do sistema Maas. Nessa p\u00e1gina deve haver uma marca de sele\u00e7\u00e3o verde ao lado dos itens 'regiond' at\u00e9 'dhcpd':</p> <p></p>"},{"location":"roteiro1/main/#tarefa-5-comissionando-os-servidores","title":"Tarefa 5: Comissionando os servidores","text":"<p>Agora \u00e9 o momento de cadastrar (fazer o host) de todas as m\u00e1quinas do server1 at\u00e9 o server5 atrav\u00e9s da aba Machines do dashboard do MaaS. Para isso algumas instru\u00e7\u00f5es devem ser seguidas:</p> <ul> <li>Ao cadastrar as m\u00e1quinas devemos preencher as op\u00e7\u00f5es:</li> <li>Preencher a op\u00e7\u00e3o Power Type com Intel AMT</li> <li>Inserir MacAddress da m\u00e1quina respectiva, esse valor est\u00e1 escrito em cada m\u00e1quina na parte de baixo</li> <li>Inserir a senha como CloudComp6s!</li> <li>IP do AMT = 172.16.15.X (sendo X o id do server, por exemplo server1 = 172.16.15.1)</li> </ul> <p>Em seguida, devemos checar todos os n\u00f3s e verificar se est\u00e3o todos com o status Ready, al\u00e9m de verificar tamb\u00e9m o hardware como mem\u00f3ria, SSD etc.</p> <p></p> <p>Ainda resta um \u00faltimo dispositivo para cadastrar, neste caso o roteador. Adicionamos o roteador pela aba Devices do dashboard do MaaS:</p> <p></p>"},{"location":"roteiro1/main/#tarefa-6-criando-ovs-bridge","title":"Tarefa 6: Criando OVS bridge","text":"<p>Uma Open vSwitch (OVS) bridge reduz a necessidade de duas interfaces de rede f\u00edsicas. As pontes OVS s\u00e3o criadas na aba NetWork ao configurar um n\u00f3 (machine). Aqui, vamos criar uma ponte a partir da interface regular 'eth0'. O nome da ponte vai ser referenciado em outras partes e como exig\u00eancia da disciplina iremos chamar de 'br-ex':</p> <p></p> <p>Devemos fazer esse procedimento para cada uma das machines da nossa nuvem.</p>"},{"location":"roteiro1/main/#tarefa-7-fazendo-acesso-remoto-ao-kit","title":"Tarefa 7: Fazendo acesso remoto ao kit","text":"<p>Agora vamos realizar um NAT para permitir o acesso da \"Rede Wi-fi Insper\" do computador pessoal ao servidor MAIN, a ideia \u00e9 utilizar a porta 22. Al\u00e9m disso, temos que configurar uma porta para acessar o MaaS remotamente, usaremos a porta 5240 e configurar no roteador na aba Transmisson -&gt; NAT:</p> <p></p> <p>Tamb\u00e9m \u00e9 necess\u00e1rio liberar o acesso ao gerenciamento remoto do roteador criando uma regra de gest\u00e3o para a rede 0.0.0.0/0, na aba System Tools -&gt; Admin Setup -&gt; Remote Management:</p> <p></p> <p>Assim j\u00e1 \u00e9 poss\u00edvel acessar remotamente sem precisar conectar o cabo Ethernet diretamente no Switch. Neste caso, precisamos utilizar um IP diferente dispon\u00edvel no dashboard do roteador na p\u00e1gina principal em WAN IPv4 -&gt; WAN1 -&gt; IP Address</p>"},{"location":"roteiro1/main/#bare-metal-django-em-nuvem","title":"Bare Metal: Django em nuvem","text":"<p>Com a infra pronta agora vamos fazer deploy de uma aplica\u00e7\u00e3o Django. Mas antes, precisamos configurar um ajuste no DNS, dentro da aba Subnets clicar na subnet 172.16.0.0/20 e editar a Subnet summary colocando o DNS do Insper - 172.20.129.131</p> <p></p>"},{"location":"roteiro1/main/#primeira-parte-banco-de-dados","title":"Primeira Parte: Banco de Dados","text":"<p>Primeiro vamos acessar o dashboard do MaaS na aba Machines e fazer deploy do ubuntu 24.04 no server1. Acessando o dashboard do server1 e em \"Take Action\" selecionar a vers\u00e3o do Ubuntu 24 e habilitar o script para inserir a chave ssh que foi obtida anteriormente e pode ser recuperada com o comando: </p> <pre><code>cat ./.ssh/id_rsa.pub\n</code></pre> <p></p> <p>Em seguida acessamos o server1 pelo terminal via SSH. Primeiro acessamos a main:</p> <pre><code>ssh cloud@10.103.1.31\n</code></pre> <p>Esse IP \u00e9 para acesso remoto pela rede do Insper</p> <p>Depois acessamos o server1:</p> <pre><code>ssh ubuntu@172.16.8.196\n</code></pre> <p>Esse IP \u00e9 o que aparece no dashboard do MaaS na aba Machines e aparece logo abaixo do nome de cada server</p> <p>Agora dentro do server1 vamos dar continuidade com o bando de dados, para isso executamos os comandos:</p> <pre><code>sudo apt update\nsudo apt install postgresql postgresql-contrib -y\n</code></pre> <p>Ainda dentro do server1 precisamos criar um usu\u00e1rio:</p> <pre><code>sudo su - postgres\ncreateuser -s cloud -W\n</code></pre> <p>Utilizar a senha cloud</p> <p>Agora vamos criar o database:</p> <pre><code>createdb -O cloud tasks\n</code></pre> <p>Agora expor o servi\u00e7o para acesso e remover o coment\u00e1rio e substituir a string da linha para aceitar conex\u00f5es remotas:</p> <p><pre><code>nano /etc/postgresql/16/main/postgresql.conf\n</code></pre> <pre><code>listen_addresses = '*'\n</code></pre></p> <p>Depois vamos liberar para qualquer m\u00e1quina dentro da subnet do kit:</p> <p><pre><code>nano /etc/postgresql/&lt;vers\u00e3o&gt;/main/pg_hba.conf\n</code></pre> Alterando a linha:</p> <pre><code>  host    all             all             172.16.0.0/20          trust\n</code></pre> <p>Em seguida, vamos sair do usu\u00e1rio postgres, liberar o firewall e reiniciar o servi\u00e7o:</p> <pre><code>sudo ufw allow 5432/tcp\nsudo systemctl restart postgresql\n</code></pre> <p>\u00c9 importante verificar o status do database e checar se h\u00e1 erros. Para isso executamos os comandos:</p> <pre><code>sudo systemctl status postgresql\n\ntelnet localhost 5432\n\ntelnet 172.16.0.196 5432\n\nsudo ss -tulnp | grep postgres\n</code></pre> <p></p>"},{"location":"roteiro1/main/#parte-2-deploy-manual-do-django","title":"Parte 2: Deploy manual do django","text":"<p>Agora vamos retornar ao dashboard do MAAS para fazer o deploy do server2. Acessando o dashboard do server2 e em \"Take Action\" selecionar a vers\u00e3o do Ubuntu 24 e habilitar o script para inserir a chave ssh que foi obtida anteriormente e pode ser recuperada com o comando: </p> <pre><code>cat ./.ssh/id_rsa.pub\n</code></pre> <p>Em seguida, acessar o SSH do server2 e clonar o seguinte reposit\u00f3rio:</p> <pre><code>git clone https://github.com/raulikeda/tasks.git\n</code></pre> <p>Esse reposit\u00f3rio tasks tem algumas atividades pr\u00e9-configuradas que iremos utilizar para preparar todo o nosso ambiente do server2. Esse reposit\u00f3rio:</p> <ul> <li> <p>Atualiza o sistema (apt update e apt upgrade).</p> </li> <li> <p>Instala pacotes essenciais como Git, curl, snapd, etc.</p> </li> <li> <p>Instala ferramentas como Django, Python, Postgres.</p> </li> </ul> <p>Agora entrando no diret\u00f3rio e entrando na pasta tasks:</p> <pre><code>cd tasks\n</code></pre> <p>Executamos o comando:</p> <pre><code>./install.sh\n</code></pre> <p>Com isso todo o ambiente do server2 est\u00e1 pronto e com a aplica\u00e7\u00e3o django instalada.</p> <p>Podemos acessar a aplica\u00e7\u00e3o django utilizando um t\u00fanel SSH e com esse servi\u00e7o tempor\u00e1rio podemos usar a aplica\u00e7\u00e3o fora do kit enquanto o terminal que o tunnel estiver utilizando esteja ativo. Para isso, sa\u00edmos da main e entramos novamente com o seguinte comando utilizando tunnel:</p> <pre><code>ssh cloud@10.103.0.1 -L 8001:[172.16.8.120]:8080\n</code></pre> <p>O comando acima ir\u00e1 criar um tunel do servi\u00e7o do server2 na porta 8080 para o seu localhost na porta 8001 usando a conex\u00e3o SSH. Agora basta acessar o navegador pelo endere\u00e7o http://localhost:8001/admin/ usando login e senha \"cloud\" e ver a aplica\u00e7\u00e3o funcionando:</p> <p></p> <p>Agora \u00e9 importante temos uma vis\u00e3o macro de como est\u00e3o as m\u00e1quinas at\u00e9 agora, principalmente seus testes de hardware e comissioning e verificar se est\u00e1 tudo OK antes de prosseguir com a pr\u00f3xima tarefa:</p> <ul> <li>Todas as m\u00e1quinas</li> </ul> <p></p> <ul> <li>Imagens sincronizadas</li> </ul> <p></p> <ul> <li>Server1</li> </ul> <p> </p> <ul> <li>Server2</li> </ul> <p> </p> <ul> <li>Server3</li> </ul> <p> </p> <ul> <li>Server4</li> </ul> <p> </p> <ul> <li>Server5</li> </ul> <p> </p>"},{"location":"roteiro1/main/#parte-3-automatizacao-de-deploy","title":"Parte 3: Automatiza\u00e7\u00e3o de deploy","text":"<p>Fizemos uma aplica\u00e7\u00e3o django no server 2, mas agora teremos 2 aplica\u00e7\u00f5es junto com o server3 compartilhando o mesmo banco de dados do server1. Isso \u00e9 essencial porque se um node cair o outro est\u00e1 no ar, para que nosso cliente acesse. Al\u00e9m disso, \u00e9 poss\u00edvel balancear os acessos entre as duas aplica\u00e7\u00f5es. Para essa nova instala\u00e7\u00e3o ser\u00e1 feito de forma autom\u00e1tica atrav\u00e9s do gerenciador de deplou Ansible.</p> <p>Seu principal diferencial \u00e9 ser idempotente, ou seja, conseguir repetir todos os procedimentos sem afetar os estados intermedi\u00e1rios da insta\u00e7\u00e3o. E para dar seguimento faremos deploy do server3 no dashboard do MAAS e acessar o SSH da main e executar os comandos:</p> <pre><code>sudo apt install ansible\nwget https://raw.githubusercontent.com/raulikeda/tasks/master/tasks-install-playbook.yaml\nansible-playbook tasks-install-playbook.yaml --extra-vars server=[IP server3]\n</code></pre> <p>Onde tem IP server3 retire os colchetes e coloque o IP</p> <p>Mais uma vez \u00e9 importante verificar os status das aplica\u00e7\u00f5es e do dashboard do MAAS:</p> <ul> <li>M\u00e1quinas</li> </ul> <p></p> <ul> <li>Aplica\u00e7\u00e3o django rodando no server2</li> </ul> <p></p> <ul> <li>Aplica\u00e7\u00e3o django rodando no server3</li> </ul> <p></p> <p>Instalar manualmente o Django envolve baixar e configurar cada componente, como Python, pip, virtualenv e as depend\u00eancias espec\u00edficas do projeto, al\u00e9m de configurar o ambiente de desenvolvimento, o que pode ser trabalhoso e propenso a erros. Por outro lado, usar o Ansible para instalar o Django permite automatizar todo o processo, criando playbooks que definem as configura\u00e7\u00f5es necess\u00e1rias, como instalar pacotes, configurar o ambiente virtual, clonar o reposit\u00f3rio do projeto e ajustar as configura\u00e7\u00f5es do Django, tornando o processo mais r\u00e1pido, al\u00e9m de facilitar a replica\u00e7\u00e3o em diferentes ambientes.</p>"},{"location":"roteiro1/main/#parte-4-balanceamento-de-carga","title":"Parte 4: Balanceamento de carga","text":"<p>Para essa \u00faltima tarefa utilizaremos uma aplica\u00e7\u00e3o de proxy reverso como load balancer. O balanceamento de carga \u00e9 uma t\u00e9cnica eficaz para distribuir o tr\u00e1fego de entrada entre v\u00e1rios servidores, garantindo que nenhum deles fique sobrecarregado. Ao dividir o processamento entre v\u00e1rias m\u00e1quinas, voc\u00ea cria uma rede mais resiliente e est\u00e1vel, capaz de lidar com falhas e manter a aplica\u00e7\u00e3o funcionando sem interrup\u00e7\u00f5es. O algoritmo Round Robin \u00e9 uma abordagem simples e eficaz para alcan\u00e7ar isso, direcionando os visitantes para diferentes endere\u00e7os IP de forma rotativa.</p> <p>Para prosseguir com a configura\u00e7\u00e3o do loadbalancing do nginx temos que instalar o Nginx com o comando:</p> <pre><code>sudo apt-get install nginx\n</code></pre> <p>Para configurar um loadbalancer round robin, precisaremos usar o m\u00f3dulo upstream do nginx. Incorporaremos a configura\u00e7\u00e3o nas defini\u00e7\u00f5es do nginx. Para isso acessamos atrav\u00e9s do comando:</p> <pre><code>sudo nano /etc/nginx/sites-available/default\n</code></pre> <p>Dentro do arquivo vamos alterar a se\u00e7\u00e3o com o nome \"upstream backend\" com a seguinte estrutura:</p> <pre><code>upstream backend {\n   server backend1;\n   server backend2;\n   server backend3;\n}\n</code></pre> <p>No nosso caso, colocamos os IPs de cada m\u00e1quina:</p> <pre><code>upstream backend {\n   server 172.16.0.196;\n   server 172.16.8.120;\n   server 172.16.8.124;\n}\n</code></pre> <p>Ainda dentro do documento iremos alterar um m\u00f3dulo com o nome de \"server\" com a seguinte configura\u00e7\u00e3o:</p> <pre><code>server {\n   location / { proxy_pass http://backend;}\n}\n</code></pre> <p>Salve o arquivo e feche com Ctrl+O e Ctrl+X e em seguida reiniciar o nginx:</p> <pre><code>sudo service nginx restart\n</code></pre> <p>Para verificar se tudo est\u00e1 corretamente configurado vamos acessar cada m\u00e1quina e entrar na pasta tasks e no arquivo views.py e alterar a mensagem \"Hello World\" para identificar cada server:</p> <pre><code>from django.shortcuts import render\n\nfrom django.http import HttpResponse\n\ndef index(request):\n\n  return HttpResponse(\"Agora estou no server2 da Cloud-V\")\n</code></pre> <p>Agora vamos realizar um request GET para testar se tudo funciona. Para isso vamos fazer um t\u00fanel com a seguinte estrutura:</p> <pre><code>ssh cloud@(IPMAIN) -L 8081:(IPSERVER):80\n</code></pre> <p>No nosso caso para cada server:</p> <pre><code>ssh cloud@10.103.1.31 -L 8081:172.16.0.196:80\n</code></pre> <pre><code>ssh cloud@10.103.1.31 -L 8081:172.16.8.120:80\n</code></pre> <p>Com isso, basta acessar o navegador pelo link http://localhost:8081/tasks/ e se a mensagem identificadora de cada server aparecer est\u00e1 tudo ok:</p> <ul> <li>Server2</li> </ul> <p></p> <ul> <li>Server3</li> </ul> <p></p>"},{"location":"roteiro2/main/","title":"Objetivos","text":"<ul> <li>Entender os conceitos b\u00e1sicos sobre uma plataforma de gerenciamento de aplica\u00e7\u00f5es distribu\u00eddas.</li> <li>Entender os conceitos b\u00e1sicos de comunica\u00e7\u00e3o entre aplica\u00e7\u00f5es e servi\u00e7os.</li> </ul>"},{"location":"roteiro2/main/#infra","title":"Infra","text":""},{"location":"roteiro2/main/#criando-uma-infraestrutura-para-deploy-com-o-juju","title":"Criando uma infraestrutura para deploy com o Juju","text":"<p>Primeiramente, acessamos a main via SSH e usamos o seguinte comando para instalar o Juju</p> <pre><code>sudo snap install juju --channel 3.6\n</code></pre> <p>Na sequ\u00eancia, precisamos adicionar o cluster MAAS para que o Juju possa gerenci\u00e1-lo como uma Cloud. Fazemos isso atrav\u00e9s de um arquivo de defini\u00e7\u00e3o de cloud, como maas-cloud.yaml.</p> <p>Criando o arquivo:</p> <pre><code>nano maas-cloud.yaml\n</code></pre> <p>Adicionando as informa\u00e7\u00f5es necess\u00e1rias:</p> <p></p> <p>Em seguida, adicionamos a Cloud ao Juju com o seguinte comando:</p> <pre><code>juju add-cloud --client -f maas-cloud.yaml maas-one\n</code></pre> <p>\u00c9 necess\u00e1rio, ainda, vincular um arquivo de credenciais para que o Juju possa integragir com a cloud adicionada. Vamos novamente usar um arquivo para importar nossas informa\u00e7\u00f5es, como maas-creds.yaml:</p> <p>Criando o arquivo:</p> <pre><code>nano maas-creds.yaml\n</code></pre> <p>Adicionando as informa\u00e7\u00f5es necess\u00e1rias:</p> <p></p> <p>Em seguida, vinculamos as credenciais \u00e0 cloud:</p> <pre><code>juju add-credential --client -f maas-creds.yaml maas-one\n</code></pre> <p>Para finalizar, criamos o controlador do server1, rotulando a m\u00e1quina via MAAS com o r\u00f3tulo \"juju\" e executamos o seguinte comando:</p> <pre><code>juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller\n</code></pre> <p>Que faz o deploy da aplica\u00e7\u00e3o do controlador, \"maas-controller\", no server1.</p> <p></p>"},{"location":"roteiro2/main/#app","title":"App","text":""},{"location":"roteiro2/main/#instalacao-do-dashboard-do-juju","title":"Instala\u00e7\u00e3o do Dashboard do Juju","text":"<p>Para fazer o deploy do dashboard, primeiro mudamos para o modelo do controlador:</p> <pre><code>juju switch maas-controller:admin/maas\n</code></pre> <p>Em seguida, fazemos o deploy do dashboard especificando a m\u00e1quina que queremos alocar:</p> <pre><code>juju deploy juju-dashboard dashboard --to lxd:0\n</code></pre> <p>Ent\u00e3o, integramos o controlador ao dashboard:</p> <pre><code>juju integrate dashboard controller\n</code></pre> <p>Por fim, expomos o painel:</p> <pre><code>juju expose dashboard\n</code></pre> <p>Todo o processo pode levar alguns minutos e podemos checar como est\u00e1 o progresso com:</p> <pre><code>juju status\n</code></pre> <p>Estando pronto agora j\u00e1 podemos acessar o Juju Dashboard com o comando:</p> <p><pre><code>juju dashboard\n---\n\n### Deploy do Grafana com Prometheus usando Juju\n\nO **Grafana** \u00e9 uma plataforma de c\u00f3digo aberto que permite visualizar dados em tempo real por meio de gr\u00e1ficos e pain\u00e9is. Para funcionar corretamente, ele precisa de um banco de dados que armazene suas configura\u00e7\u00f5es e metadados. Neste caso, vamos utilizar o **Prometheus**.\n\n---\n\n#### 1. Criar a pasta para armazenar os charms\n\nPrimeiro, criamos uma pasta chamada `charms` onde vamos baixar os charms do Grafana e do Prometheus:\n</code></pre> mkdir -p /home/cloud/charms cd /home/cloud/charms <pre><code>---\n\n#### 2. Baixar os charms do Charmhub\n\nAgora, vamos fazer o download dos charms necess\u00e1rios:\n</code></pre> juju download grafana juju download prometheus2 <pre><code>---\n\n#### 3. Fazer o deploy dos charms\n\nCom os arquivos baixados, fazemos o deploy local dos charms:\n\n**Deploy do Prometheus:**\n</code></pre> juju deploy ./prometheus2_r69.charm <pre><code>**Deploy do Grafana:**\n</code></pre> juju deploy ./grafana_r69.charm <pre><code>---\n\n#### 4. Integrar Grafana com Prometheus\n\nCom os servi\u00e7os no ar, integramos o Grafana ao Prometheus:\n</code></pre> juju integrate grafana prometheus2 <pre><code>---\n\n#### 5. Acompanhar o status do deploy\n\nPara acompanhar o andamento dos deploys e da integra\u00e7\u00e3o em tempo real:\n</code></pre> watch -n 1 juju status ```</p>"},{"location":"roteiro2/main/#tarefas","title":"Tarefas","text":"<ol> <li>Print da tela do Dashboard do MAAS com as Maquinas e seus respectivos IPs.</li> </ol> <ol> <li>Print de tela do comando \"juju status\" depois que o Grafana est\u00e1 \"active\".</li> </ol> <ol> <li>Print da tela do Dashboard do Grafana com o Prometheus aparecendo como source.</li> </ol> <ol> <li>Print do acesso ao Dashboard via network Insper.</li> </ol> <ol> <li>Print na tela que mostra as aplica\u00e7\u00f5es sendo gerenciadas pelo JUJU.</li> </ol>"}]}